import asyncio
import os
import sys
import json
from datetime import datetime
from dotenv import load_dotenv

# Add the current directory to sys.path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Load env vars
load_dotenv(os.path.join(os.path.dirname(os.path.abspath(__file__)), '.env'))

from app.agents.objective import objective_agent


async def save_voting_results(voting_result: dict, topic: str, case_study: str):
    """Save MAKER voting results to organized project folder."""
    # Create safe project name from topic
    project_name = "".join(c if c.isalnum() or c in (' ', '-', '_') else '_' for c in topic)
    project_name = "_".join(project_name.split()).lower()[:50]  # Limit length
    
    # Organize: thesis_data/{project_name}/voting/{timestamp}/
    base_dir = f"../thesis_data/{project_name}/voting"
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    voting_dir = f"{base_dir}/{timestamp}"
    
    os.makedirs(voting_dir, exist_ok=True)
    
    print(f"\nğŸ’¾ SAVED VOTING RESULTS")
    print(f"   Project: {project_name}")
    print(f"   Session: {timestamp}")
    
    # Save full voting data
    full_file = f"{voting_dir}/full_voting_session.json"
    save_data = {
        "topic": topic,
        "case_study": case_study,
        "timestamp": timestamp,
        **voting_result
    }
    with open(full_file, 'w') as f:
        json.dump(save_data, f, indent=2)
    print(f"   âœ“ Full data: {full_file}")
    
    # Save final objectives separately for easy access
    objectives_file = f"{voting_dir}/final_objectives.json"
    objectives_data = {
        "topic": topic,
        "case_study": case_study,
        "objectives": voting_result["objectives"],
        "validation": voting_result["validation"],
        "voting_stats": voting_result["voting_stats"],
        "cost": voting_result["actual_cost"],
        "timestamp": timestamp
    }
    with open(objectives_file, 'w') as f:
        json.dump(objectives_data, f, indent=2)
    print(f"   âœ“ Final objectives: {objectives_file}")
    
    # Create readable markdown report
    md_file = f"{voting_dir}/REPORT.md"
    with open(md_file, 'w') as f:
        f.write(f"# MAKER Voting Session Report\n\n")
        f.write(f"**Session ID:** `{timestamp}`\n\n")
        f.write(f"**Topic:** {topic}\n\n")
        f.write(f"**Case Study:** {case_study}\n\n")
        f.write(f"**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"**Mode:** MAKER Framework (First-to-ahead-by-k Voting)\n\n")
        f.write("---\n\n")
        
        # Final objectives
        f.write(f"## ğŸ¯ Final Objectives\n\n")
        for i, obj in enumerate(voting_result["objectives"], 1):
            f.write(f"{i}. {obj}\n\n")
        
        # Voting statistics
        stats = voting_result["voting_stats"]
        f.write(f"## ğŸ“Š Voting Statistics\n\n")
        f.write(f"- **k-threshold:** {stats['k_threshold']}\n")
        f.write(f"- **Total samples:** {stats['total_samples']}\n")
        f.write(f"- **Flagged samples:** {stats['flagged_samples']}\n")
        f.write(f"- **Valid samples:** {stats['total_samples'] - stats['flagged_samples']}\n")
        f.write(f"- **Convergence rounds:** {stats['convergence_rounds']}\n")
        f.write(f"- **Winner votes:** {stats['winner_votes']}\n")
        f.write(f"- **Flag rate:** {stats['flagged_samples'] / stats['total_samples'] * 100:.1f}%\n\n")
        
        # Vote distribution
        if stats.get('vote_distribution'):
            f.write(f"### Vote Distribution\n\n")
            for candidate, votes in stats['vote_distribution'].items():
                f.write(f"- Candidate: {votes} votes\n")
            f.write("\n")
        
        # Cost analysis
        f.write(f"## ğŸ’° Cost Analysis\n\n")
        cost_est = voting_result["cost_estimate"]
        f.write(f"- **Total cost:** ${voting_result['actual_cost']:.4f}\n")
        f.write(f"- **Cost per objective:** ${cost_est['cost_per_objective']:.4f}\n")
        f.write(f"- **Samples per objective:** {cost_est['samples_per_objective']}\n\n")
        
        # Validation results
        validation = voting_result["validation"]
        f.write(f"## âœ… Validation Results\n\n")
        f.write(f"- **Valid:** {'Yes' if validation['is_valid'] else 'No'}\n")
        f.write(f"- **Overall score:** {validation.get('overall_score', 'N/A')}/100\n\n")
        
        if validation.get('strengths'):
            f.write(f"### Strengths\n\n")
            for s in validation['strengths']:
                f.write(f"- {s}\n")
            f.write("\n")
        
        if validation.get('issues'):
            f.write(f"### Issues\n\n")
            for issue in validation['issues']:
                f.write(f"- **[{issue['severity']}]** {issue['issue']}\n")
                if issue.get('suggestion'):
                    f.write(f"  - Suggestion: {issue['suggestion']}\n")
            f.write("\n")
        
        # Red flag analysis
        if stats['flagged_samples'] > 0:
            f.write(f"## ğŸš© Red Flag Analysis\n\n")
            f.write(f"Total flagged: {stats['flagged_samples']} out of {stats['total_samples']} samples\n\n")
            
            # Analyze flag reasons from all_votes
            flag_reasons = {}
            for vote in stats.get('all_votes', []):
                if vote.get('flagged'):
                    # This would need to be enhanced to track flag reasons
                    pass
            f.write("\n")
        
        # Efficiency metrics
        f.write(f"## ğŸ“ˆ Efficiency Metrics\n\n")
        efficiency = stats['convergence_rounds'] / stats['total_samples'] * 100
        f.write(f"- **Efficiency:** {efficiency:.1f}% (valid votes / total samples)\n")
        f.write(f"- **Avg samples per consensus:** {stats['total_samples'] / len(voting_result['objectives']):.1f}\n")
        
        # Comparison to single-shot
        single_shot_cost = 0.05 * len(voting_result['objectives'])
        cost_multiplier = voting_result['actual_cost'] / single_shot_cost
        f.write(f"\n### vs Single-Shot\n\n")
        f.write(f"- **Single-shot cost:** ${single_shot_cost:.4f}\n")
        f.write(f"- **MAKER cost:** ${voting_result['actual_cost']:.4f}\n")
        f.write(f"- **Cost multiplier:** {cost_multiplier:.2f}x\n")
        f.write(f"- **Quality improvement:** Expected 60% â†’ 95% SMART adherence\n\n")
    
    print(f"   âœ“ Markdown report: {md_file}")
    
    # Create project summary file
    summary_file = f"../thesis_data/{project_name}/PROJECT_INFO.md"
    if not os.path.exists(summary_file):
        with open(summary_file, 'w') as f:
            f.write(f"# Project: {topic}\n\n")
            f.write(f"**Case Study:** {case_study}\n\n")
            f.write(f"**Created:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")
            f.write("## MAKER Voting Sessions\n\n")
            f.write(f"- [{timestamp}](voting/{timestamp}/REPORT.md) - {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
        print(f"   âœ“ Project info: {summary_file}")
    else:
        # Append to existing project summary
        with open(summary_file, 'a') as f:
            f.write(f"- [{timestamp}](voting/{timestamp}/REPORT.md) - {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
        print(f"   âœ“ Updated project info: {summary_file}")
    
    print(f"\nğŸ“ All files saved to: ../thesis_data/{project_name}/\n")


async def main():
    print("\nğŸ¯ MAKER FRAMEWORK - VOTING-BASED OBJECTIVE GENERATION")
    print("=" * 70)
    print("\nThis uses the MAKER framework to generate objectives with:")
    print("  â€¢ First-to-ahead-by-k voting (default k=3)")
    print("  â€¢ Red-flagging to discard unreliable responses")
    print("  â€¢ Zero-error objective generation")
    print("\nBenefits:")
    print("  âœ“ >95% SMART criteria adherence")
    print("  âœ“ Zero methodology creep")
    print("  âœ“ Reduced regeneration cycles")
    print("  âœ“ Comprehensive validation")
    print("\n" + "=" * 70 + "\n")
    
    topic = input("Enter Research Topic: ").strip()
    case_study = input("Enter Case Study: ").strip()
    
    if not topic:
        print("âŒ Topic is required!")
        return
    
    methodology = input("Methodology (Quantitative/Qualitative/Mixed, or leave blank): ").strip()
    if not methodology:
        methodology = None
    
    # Ask for k threshold
    k_input = input("k-threshold (default 3, press Enter to use default): ").strip()
    k = int(k_input) if k_input.isdigit() else 3
    
    # Ask about red flags
    red_flags_input = input("Enable red-flagging? (Y/n, default Y): ").strip().lower()
    enable_red_flags = not red_flags_input.startswith('n')
    
    print(f"\nğŸš€ Starting MAKER voting session...")
    print(f"   k-threshold: {k}")
    print(f"   Red-flagging: {'Enabled' if enable_red_flags else 'Disabled'}\n")
    
    try:
        result = await objective_agent.generate_objectives_with_voting(
            topic=topic,
            case_study=case_study,
            methodology=methodology,
            k=k,
            enable_red_flags=enable_red_flags,
            thesis_id=None  # No thesis_id for standalone testing
        )
        
        # Display final objectives
        print("\n" + "=" * 70)
        print("ğŸ¯ FINAL OBJECTIVES")
        print("=" * 70 + "\n")
        for i, obj in enumerate(result["objectives"], 1):
            print(f"{i}. {obj}\n")
        
        # Display voting stats
        stats = result["voting_stats"]
        print("=" * 70)
        print("ğŸ“Š VOTING STATISTICS")
        print("=" * 70)
        print(f"   Total samples: {stats['total_samples']}")
        print(f"   Flagged: {stats['flagged_samples']}")
        print(f"   Valid: {stats['total_samples'] - stats['flagged_samples']}")
        print(f"   Convergence rounds: {stats['convergence_rounds']}")
        print(f"   Winner votes: {stats['winner_votes']}")
        print(f"   Flag rate: {stats['flagged_samples'] / stats['total_samples'] * 100:.1f}%\n")
        
        # Display validation
        validation = result["validation"]
        print("=" * 70)
        print("âœ… VALIDATION")
        print("=" * 70)
        print(f"   Valid: {'Yes' if validation['is_valid'] else 'No'}")
        print(f"   Score: {validation.get('overall_score', 'N/A')}/100\n")
        
        if validation.get('strengths'):
            print("   Strengths:")
            for s in validation['strengths']:
                print(f"     âœ“ {s}")
            print()
        
        if validation.get('issues'):
            print("   Issues:")
            for issue in validation['issues']:
                print(f"     âš  [{issue['severity']}] {issue['issue']}")
            print()
        
        # Display cost
        print("=" * 70)
        print("ğŸ’° COST")
        print("=" * 70)
        print(f"   Total: ${result['actual_cost']:.4f}")
        print(f"   Per objective: ${result['cost_estimate']['cost_per_objective']:.4f}\n")
        
        # Ask to save
        save = input("\nğŸ’¾ Save voting results? (y/n): ").strip().lower()
        
        if save.startswith('y'):
            await save_voting_results(result, topic, case_study)
            print("\nâœ… Voting results saved successfully!")
        else:
            print("\nâŒ Results not saved.")
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
