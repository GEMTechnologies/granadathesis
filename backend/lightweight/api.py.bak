"""
Lightweight Thesis API - Complete FastAPI Application
Main API file with all endpoints for the lightweight thesis system.
"""
import os
import json
import asyncio
import base64
import uuid
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
from pydantic import BaseModel, Field

from fastapi import FastAPI, HTTPException, Request, Query, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, Response, StreamingResponse
from sse_starlette.sse import EventSourceResponse
import mimetypes

# ============================================================================
# FastAPI APP INITIALIZATION
# ============================================================================

app = FastAPI(
    title="Lightweight Thesis API",
    description="Complete API for lightweight thesis generation system",
    version="1.0.0"
)

# CORS Middleware - MUST be before routes
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:3001",
        "http://127.0.0.1:3000",
        "http://127.0.0.1:3001",
        "*"  # Allow all for development
    ],
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS", "PATCH", "HEAD"],
    allow_headers=["*", "Cache-Control", "Content-Type", "Last-Event-ID"],
    expose_headers=["*"],
)

# GraphQL for fast source referencing
try:
    from strawberry.fastapi import GraphQLRouter
    from graphql.sources_schema import schema as sources_schema
    graphql_app = GraphQLRouter(sources_schema)
    app.include_router(graphql_app, prefix="/graphql")
    print("âœ… GraphQL enabled at /graphql")
except Exception as e:
    print(f"âš ï¸ GraphQL not available: {e}")

# ============================================================================
# SERVICE IMPORTS
# ============================================================================

from services.workspace_service import workspace_service, WORKSPACES_DIR
from services.session_service import session_service
from services.planner import planner_service
from services.chat_history_service import chat_history_service
from services.skills_manager import get_skills_manager
from core.events import events

# ============================================================================
# REQUEST/RESPONSE MODELS
# ============================================================================

class ChatMessageRequest(BaseModel):
    message: str
    session_id: Optional[str] = "default"
    workspace_id: Optional[str] = "default"
    user_id: Optional[str] = "default"
    mentioned_agents: Optional[List[str]] = Field(default_factory=list)

class SessionInitRequest(BaseModel):
    user_id: Optional[str] = "user-1"
    workspace_id: Optional[str] = None

class CreateFileRequest(BaseModel):
    name: str
    path: Optional[str] = None
    content: str = ""

class CreateFolderRequest(BaseModel):
    name: str
    path: Optional[str] = None

class RenameRequest(BaseModel):
    old_path: str
    new_path: str

# ============================================================================
# WORKSPACE FILE LISTING (Recursive)
# ============================================================================

def list_workspace_files(workspace_id: str, base_path: str = "") -> List[Dict]:
    """Recursively list all files and folders in workspace."""
    workspace_path = WORKSPACES_DIR / workspace_id
    
    if not workspace_path.exists():
        print(f"âš ï¸ Workspace {workspace_id} not found at {workspace_path}")
        return []
    
    items = []
    search_path = workspace_path / base_path if base_path else workspace_path
    
    if not search_path.exists():
        print(f"âš ï¸ Search path {search_path} not found")
        return []
        
    # Directories to ignore to prevent hanging on massive dependency trees
    IGNORE_DIRS = {
        'node_modules', 'venv', '.venv', '__pycache__', '.git', 
        '.next', 'dist', 'build', 'coverage'
    }
    
    print(f"ðŸ“‚ Listing files for workspace {workspace_id} in {search_path}")
    
    try:
        # Walk manually to control recursion into ignored directories
        for root, dirs, files in os.walk(search_path):
            # Modify dirs in-place to skip ignored directories
            dirs[:] = [d for d in dirs if d not in IGNORE_DIRS and not d.startswith('.')]
            
            root_path = Path(root)
            
            # Add directories
            for d in dirs:
                full_path = root_path / d
                try:
                    rel_path = str(full_path.relative_to(workspace_path))
                    stat = full_path.stat()
                    items.append({
                        "name": d,
                        "path": rel_path.replace("\\", "/"),
                        "type": "folder",
                        "size": 0,
                        "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                        "created": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                    })
                except ValueError:
                    continue # Should not happen given we walk inside workspace_path
            
            # Add files
            for f in files:
                if f.startswith('.') or f == "workspace.json":
                    continue
                    
                full_path = root_path / f
                try:
                    rel_path = str(full_path.relative_to(workspace_path))
                    stat = full_path.stat()
                    items.append({
                        "name": f,
                        "path": rel_path.replace("\\", "/"),
                        "type": "file",
                        "size": stat.st_size,
                        "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                        "created": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                    })
                except ValueError:
                    continue
                    
    except Exception as e:
        print(f"âŒ Error listing files: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"âœ… Found {len(items)} items in {workspace_id}")
    return items

# ============================================================================
# TOOL EXECUTION FUNCTIONS
# ============================================================================

async def execute_tool(tool_name: str, arguments: Dict, workspace_id: str = "default") -> Dict:
    """Execute a tool and return the result."""
    workspace_path = WORKSPACES_DIR / workspace_id
    
    if tool_name == "list_files":
        path = arguments.get("path", ".")
        search_path = workspace_path / path if path else workspace_path
        
        if not search_path.exists():
            return {"status": "error", "error": f"Path '{path}' not found"}
        
        files = []
        try:
            for item in search_path.iterdir():
                if item.name.startswith('.'):
                    continue
                rel_path = str(item.relative_to(workspace_path))
                files.append({
                    "name": item.name,
                    "path": rel_path.replace("\\", "/"),
                    "type": "folder" if item.is_dir() else "file"
                })
        except Exception as e:
            return {"status": "error", "error": str(e)}
        
        return {"status": "success", "files": files}
    
    elif tool_name == "read_file":
        file_path = arguments.get("path")
        full_path = workspace_path / file_path if file_path else workspace_path
        
        if not full_path.exists() or not full_path.is_file():
            return {"status": "error", "error": f"File '{file_path}' not found"}
        
        try:
            content = full_path.read_text(encoding='utf-8')
            return {"status": "success", "content": content, "path": file_path}
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    elif tool_name == "write_file" or tool_name == "save_file":
        file_path = arguments.get("path")
        content = arguments.get("content", "")
        
        if not file_path:
            return {"status": "error", "error": "File path is required"}
        
        full_path = workspace_path / file_path
        full_path.parent.mkdir(parents=True, exist_ok=True)
        
        try:
            full_path.write_text(content, encoding='utf-8')
            return {"status": "success", "path": file_path, "message": f"File '{file_path}' saved"}
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    elif tool_name == "create_folder":
        folder_path = arguments.get("path")
        
        if not folder_path:
            return {"status": "error", "error": "Folder path is required"}
        
        full_path = workspace_path / folder_path
        
        try:
            full_path.mkdir(parents=True, exist_ok=True)
            return {"status": "success", "path": folder_path, "message": f"Folder '{folder_path}' created"}
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    elif tool_name == "delete_file":
        file_path = arguments.get("path")
        full_path = workspace_path / file_path if file_path else workspace_path
        
        if not full_path.exists():
            return {"status": "error", "error": f"Path '{file_path}' not found"}
        
        try:
            if full_path.is_file():
                full_path.unlink()
            else:
                import shutil
                shutil.rmtree(full_path)
            return {"status": "success", "message": f"Deleted '{file_path}'"}
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    elif tool_name == "web_search":
        query = arguments.get("query", "")
        try:
            from services.web_search import web_search_service
            results = await web_search_service.search(query, max_results=5)
            return {"status": "success", "results": results}
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    elif tool_name == "image_search":
        query = arguments.get("query", "")
        limit = arguments.get("limit", 5)
        try:
            from services.intelligent_image_search import intelligent_image_search_service
            results = await intelligent_image_search_service.search(query, limit=limit)
            # Format results for easier use
            formatted_results = []
            for img in results:
                if isinstance(img, dict):
                    formatted_results.append({
                        "url": img.get("url") or img.get("full") or img.get("image_url"),
                        "title": img.get("title") or img.get("description") or query,
                        "thumbnail": img.get("thumbnail") or img.get("url"),
                        "source": img.get("source", "search")
                    })
            return {"status": "success", "images": formatted_results}
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    elif tool_name == "image_generate":
        prompt = arguments.get("prompt", "")
        size = arguments.get("size", "1024x1024")
        try:
            from services.image_generation import image_generation_service
            result = await image_generation_service.generate(prompt=prompt, size=size)
            if result.get("success"):
                return {
                    "status": "success",
                    "image_url": result.get("image_url") or result.get("url"),
                    "prompt": prompt
                }
            else:
                return {"status": "error", "error": result.get("error", "Image generation failed")}
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    return {"status": "error", "error": f"Unknown tool: {tool_name}"}

# ============================================================================
# HEALTH & ROOT ENDPOINTS
# ============================================================================

@app.get("/")
async def root():
    return {"message": "Lightweight Thesis API is running", "version": "1.0.0"}

@app.get("/health")
async def health():
    return {"status": "healthy"}

# ============================================================================
# SESSION ENDPOINTS
# ============================================================================

@app.post("/api/session/init")
async def init_session(request: SessionInitRequest):
    """Initialize a new session."""
    try:
        session_data = session_service.get_or_create_session()
        
        if request.workspace_id:
            session_service.set_workspace(session_data["session_id"], request.workspace_id)
            session_data["workspace_id"] = request.workspace_id
        
        return {
            "session_id": session_data["session_id"],
            "user_id": request.user_id,
            "workspace_id": session_data.get("workspace_id"),
            "session_url": f"/session/{session_data['session_id']}",
            "has_workspace": session_data.get("workspace_id") is not None
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/session/{session_id}")
async def get_session(session_id: str):
    """Get session data. Creates session if it doesn't exist."""
    try:
        session_data = session_service.get_session(session_id)
        if not session_data:
            # Create session if it doesn't exist (for new sessions)
            session_data = session_service.get_or_create_session(session_id)
        
        return session_data
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# WORKSPACE ENDPOINTS
# ============================================================================

@app.get("/api/workspaces/list")
async def list_workspaces():
    """List all workspaces."""
    try:
        workspaces = workspace_service.list_workspaces()
        return {"workspaces": workspaces}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/chapter/background-styles")
async def get_background_styles():
    """Get available background writing styles for chapter generation."""
    from services.parallel_chapter_generator import BACKGROUND_STYLES
    return {
        "styles": [
            {
                "id": style_id,
                "name": style["name"],
                "description": style["description"],
                "best_for": style["best_for"],
                "sections": style["sections"]
            }
            for style_id, style in BACKGROUND_STYLES.items()
        ],
        "default": "inverted_pyramid"
    }

@app.get("/api/workspace/{workspace_id}/structure")
async def get_workspace_structure(workspace_id: str):
    """Get complete workspace file structure recursively."""
    try:
        if not workspace_service.workspace_exists(workspace_id):
            # Create default workspace if it doesn't exist
            await workspace_service.create_workspace(workspace_id=workspace_id)
        
        items = list_workspace_files(workspace_id)
        
        return {
            "workspace_id": workspace_id,
            "items": items
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/workspace/{workspace_id}/files/{file_path:path}")
async def get_file(workspace_id: str, file_path: str):
    """Get file content."""
    try:
        workspace_path = WORKSPACES_DIR / workspace_id / file_path
        
        if not workspace_path.exists() or not workspace_path.is_file():
            raise HTTPException(status_code=404, detail="File not found")
        
        content = workspace_path.read_text(encoding='utf-8')
        
        return {
            "path": file_path,
            "content": content,
            "name": workspace_path.name
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/workspace/{workspace_id}/serve/{file_path:path}")
async def serve_file(workspace_id: str, file_path: str):
    """Serve binary files (images, PDFs, etc.) with correct Content-Type for inline viewing."""
    try:
        workspace_path = WORKSPACES_DIR / workspace_id / file_path
        
        if not workspace_path.exists() or not workspace_path.is_file():
            raise HTTPException(status_code=404, detail="File not found")
        
        # Determine media type
        media_type, _ = mimetypes.guess_type(str(workspace_path))
        if not media_type:
            media_type = "application/octet-stream"
        
        # For viewable files (PDFs, images), use inline disposition to display in browser
        # For other files, use attachment to force download
        viewable_types = ['application/pdf', 'image/png', 'image/jpeg', 'image/gif', 'image/webp', 'image/svg+xml']
        
        if media_type in viewable_types:
            # Return without filename to display inline (not download)
            return FileResponse(
                path=str(workspace_path),
                media_type=media_type,
                headers={"Content-Disposition": f"inline; filename=\"{workspace_path.name}\""}
            )
        else:
            # Force download for other file types
            return FileResponse(
                path=str(workspace_path),
                media_type=media_type,
                filename=workspace_path.name
            )
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/workspace/{workspace_id}/files")
async def create_file(workspace_id: str, request: CreateFileRequest):
    """Create a new file."""
    try:
        workspace_path = WORKSPACES_DIR / workspace_id
        file_path = workspace_path / (request.path or request.name)
        
        file_path.parent.mkdir(parents=True, exist_ok=True)
        file_path.write_text(request.content, encoding='utf-8')
        
        return {
            "status": "success",
            "path": str(file_path.relative_to(workspace_path)),
            "name": file_path.name
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# FILE CONVERSION ENDPOINTS
# ============================================================================

class ConvertDocxRequest(BaseModel):
    content: str
    filename: Optional[str] = "document"

@app.post("/api/workspace/{workspace_id}/convert/docx")
async def convert_to_docx(workspace_id: str, request: ConvertDocxRequest):
    """Convert markdown content to DOCX."""
    try:
        from docx import Document
        from docx.shared import Pt
        import tempfile
        
        document = Document()
        
        # Simple Markdown parsing
        # This is a basic implementation. For production, consider using markdown library to parse to HTML 
        # then convert HTML to docx, but python-docx is cleaner for simple stuff.
        
        lines = request.content.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            if line.startswith('# '):
                document.add_heading(line[2:], level=1)
            elif line.startswith('## '):
                document.add_heading(line[3:], level=2)
            elif line.startswith('### '):
                document.add_heading(line[4:], level=3)
            elif line.startswith('- ') or line.startswith('* '):
                document.add_paragraph(line[2:], style='List Bullet')
            else:
                document.add_paragraph(line)
        
        # Create temp file
        workspace_path = WORKSPACES_DIR / workspace_id
        temp_dir = workspace_path / "temp"
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        filename = request.filename
        if not filename.endswith('.docx'):
            filename += '.docx'
            
        file_path = temp_dir / filename
        document.save(str(file_path))
        
        return FileResponse(
            path=str(file_path),
            filename=filename,
            media_type='application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        )
            
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

class AddSourceRequest(BaseModel):
    title: str
    authors: List[str] = []
    year: int = 2024
    type: str = "paper"
    doi: Optional[str] = None
    url: Optional[str] = None
    abstract: Optional[str] = None
    pdf_url: Optional[str] = None

class SearchSourcesRequest(BaseModel):
    query: str
    max_results: int = 5
    auto_save: bool = True  # Default to auto-save

@app.get("/api/workspace/{workspace_id}/sources")
async def list_sources(workspace_id: str):
    """List all sources in workspace."""
    try:
        from services.sources_service import sources_service
        sources = sources_service.list_sources(workspace_id)
        return {
            "workspace_id": workspace_id,
            "total": len(sources),
            "sources": sources
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/workspace/{workspace_id}/sources/{source_id}")
async def get_source(workspace_id: str, source_id: str):
    """Get a specific source."""
    try:
        from services.sources_service import sources_service
        source = sources_service.get_source(workspace_id, source_id)
        if not source:
            raise HTTPException(status_code=404, detail="Source not found")
        return source
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/workspace/{workspace_id}/sources")
async def add_source(workspace_id: str, request: AddSourceRequest):
    """Add a source manually."""
    try:
        from services.sources_service import sources_service
        source = await sources_service.add_source(workspace_id, request.model_dump())
        return {"status": "success", "source": source}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/workspace/{workspace_id}/sources/search")
async def search_and_save_sources(workspace_id: str, request: SearchSourcesRequest):
    """Search academic sources and optionally save them."""
    try:
        from services.sources_service import sources_service
        result = await sources_service.search_and_save(
            workspace_id=workspace_id,
            query=request.query,
            max_results=request.max_results,
            auto_save=request.auto_save
        )
        return result
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/workspace/{workspace_id}/sources/{source_id}/text")
async def get_source_text(workspace_id: str, source_id: str):
    """Get extracted text from a source (for LLM context)."""
    try:
        from services.sources_service import sources_service
        text = sources_service.get_source_text(workspace_id, source_id)
        if not text:
            raise HTTPException(status_code=404, detail="Source text not available")
        return {"source_id": source_id, "text": text}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/workspace/{workspace_id}/sources-context")
async def get_sources_context(workspace_id: str, max_sources: int = 5):
    """Get formatted sources context for LLM."""
    try:
        from services.sources_service import sources_service
        context = sources_service.get_sources_context(workspace_id, max_sources)
        return {"workspace_id": workspace_id, "context": context}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/workspace/{workspace_id}/sources/{source_id}")
async def delete_source(workspace_id: str, source_id: str):
    """Delete a source."""
    try:
        from services.sources_service import sources_service
        success = sources_service.delete_source(workspace_id, source_id)
        if not success:
            raise HTTPException(status_code=404, detail="Source not found")
        return {"status": "success", "deleted": source_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# WORKSPACE SETTINGS ENDPOINTS
# ============================================================================

class WorkspaceSettingsUpdate(BaseModel):
    """Model for updating workspace settings."""
    search: Optional[Dict[str, Any]] = None
    indexing: Optional[Dict[str, Any]] = None
    citations: Optional[Dict[str, Any]] = None


@app.get("/api/workspace/{workspace_id}/settings")
async def get_workspace_settings(workspace_id: str):
    """Get workspace settings (search filters, indexing, citations)."""
    try:
        from services.workspace_service import WorkspaceService
        settings = WorkspaceService.get_workspace_settings(workspace_id)
        return {
            "workspace_id": workspace_id,
            "settings": settings
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.put("/api/workspace/{workspace_id}/settings")
async def update_workspace_settings(workspace_id: str, request: WorkspaceSettingsUpdate):
    """Update workspace settings."""
    try:
        from services.workspace_service import WorkspaceService
        
        updates = {}
        if request.search:
            updates["search"] = request.search
        if request.indexing:
            updates["indexing"] = request.indexing
        if request.citations:
            updates["citations"] = request.citations
        
        success = WorkspaceService.update_workspace_settings(workspace_id, updates)
        if not success:
            raise HTTPException(status_code=404, detail="Workspace not found")
        
        # Return updated settings
        new_settings = WorkspaceService.get_workspace_settings(workspace_id)
        return {
            "status": "success",
            "workspace_id": workspace_id,
            "settings": new_settings
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/workspace/{workspace_id}/search-filters")
async def get_search_filters(workspace_id: str):
    """Get search-specific filters formatted for API consumption."""
    try:
        from services.workspace_service import WorkspaceService
        filters = WorkspaceService.get_search_filters(workspace_id)
        return {
            "workspace_id": workspace_id,
            "filters": filters
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# CONVERSATION MEMORY ENDPOINTS
# ============================================================================

class CreateConversationRequest(BaseModel):
    title: str = "New Conversation"


class AddMessageRequest(BaseModel):
    role: str  # user, assistant
    content: str
    job_id: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None


class RecallRequest(BaseModel):
    query: str


@app.post("/api/workspace/{workspace_id}/conversations")
async def create_conversation(workspace_id: str, request: CreateConversationRequest):
    """Create a new conversation with memory."""
    from services.conversation_memory import conversation_memory
    
    conversation = await conversation_memory.create_conversation(
        workspace_id=workspace_id,
        title=request.title
    )
    return {"conversation_id": conversation.conversation_id, "workspace_id": workspace_id}


@app.get("/api/workspace/{workspace_id}/conversations")
async def list_conversations(workspace_id: str):
    """List all conversations in workspace."""
    from services.conversation_memory import conversation_memory
    
    conversations = conversation_memory.list_conversations(workspace_id)
    return {"conversations": conversations, "total": len(conversations)}


@app.get("/api/workspace/{workspace_id}/conversations/{conversation_id}/messages")
async def get_messages(
    workspace_id: str, 
    conversation_id: str,
    limit: int = 50,
    offset: int = 0
):
    """Get messages from conversation."""
    from services.conversation_memory import conversation_memory
    
    messages = await conversation_memory.get_messages(
        workspace_id, conversation_id, limit=limit, offset=offset
    )
    return {"messages": messages, "count": len(messages)}


@app.post("/api/workspace/{workspace_id}/conversations/{conversation_id}/messages")
async def add_message(
    workspace_id: str,
    conversation_id: str,
    request: AddMessageRequest
):
    """Add a message to conversation history."""
    from services.conversation_memory import conversation_memory
    
    message = await conversation_memory.add_message(
        workspace_id=workspace_id,
        conversation_id=conversation_id,
        role=request.role,
        content=request.content,
        job_id=request.job_id,
        metadata=request.metadata
    )
    return {"message_id": message.id, "timestamp": message.timestamp}


@app.post("/api/workspace/{workspace_id}/conversations/{conversation_id}/recall")
async def recall_memory(
    workspace_id: str,
    conversation_id: str,
    request: RecallRequest
):
    """Recall relevant information from conversation history."""
    from services.conversation_memory import conversation_memory
    
    result = await conversation_memory.recall(
        workspace_id=workspace_id,
        conversation_id=conversation_id,
        query=request.query
    )
    return result


@app.get("/api/workspace/{workspace_id}/conversations/{conversation_id}/context")
async def get_context(workspace_id: str, conversation_id: str, message: str = ""):
    """Get context for LLM prompt including memory."""
    from services.conversation_memory import conversation_memory
    
    context = await conversation_memory.get_context_for_prompt(
        workspace_id=workspace_id,
        conversation_id=conversation_id,
        current_message=message
    )
    return {"context": context}


# ============================================================================
# MULTIMODAL FILE PROCESSING ENDPOINTS
# ============================================================================

@app.post("/api/workspace/{workspace_id}/files/process")
async def process_file(workspace_id: str, file_path: str):
    """
    Process a file and extract content.
    
    Supports: Audio (transcription), Images (vision), PDFs, DOCX, Data files
    """
    from services.multimodal_processor import multimodal_processor
    
    result = await multimodal_processor.process_file(file_path, workspace_id)
    return result


@app.post("/api/workspace/{workspace_id}/files/process-batch")
async def process_files_batch(workspace_id: str, file_paths: List[str]):
    """Process multiple files in parallel."""
    from services.multimodal_processor import multimodal_processor
    
    results = await multimodal_processor.process_uploaded_files(workspace_id, file_paths)
    return {"results": results, "processed": len(results)}


@app.post("/api/workspace/{workspace_id}/audio/transcribe")
async def transcribe_audio(workspace_id: str, file_path: str):
    """Transcribe audio file to text."""
    from services.multimodal_processor import multimodal_processor
    
    result = await multimodal_processor.process_audio(file_path)
    return result


@app.post("/api/workspace/{workspace_id}/image/analyze")
async def analyze_image(workspace_id: str, file_path: str):
    """Analyze image using vision AI."""
    from services.multimodal_processor import multimodal_processor
    
    result = await multimodal_processor.process_image(file_path)
    return result


# ============================================================================
# DOCUMENT EXPORT ENDPOINTS
# ============================================================================

class ExportDocxRequest(BaseModel):
    """Request to export content to DOCX."""
    content: str
    title: str = "Document"
    filename: Optional[str] = None


@app.post("/api/workspace/{workspace_id}/export/docx")
async def export_to_docx(workspace_id: str, request: ExportDocxRequest):
    """
    Export content to DOCX with clickable citation links.
    
    In the exported DOCX, citations like (Smith, 2020) become
    clickable hyperlinks that jump to the References section.
    """
    from services.document_exporter import document_exporter
    from services.sources_service import sources_service
    from services.workspace_service import WORKSPACES_DIR
    
    try:
        # Get sources for citation mapping
        sources = sources_service.list_sources(workspace_id)
        
        # Generate filename
        filename = request.filename or f"export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx"
        if not filename.endswith('.docx'):
            filename += '.docx'
        
        output_dir = WORKSPACES_DIR / workspace_id / "outputs"
        output_path = output_dir / filename
        
        # Export
        result_path = document_exporter.export_to_docx(
            content=request.content,
            output_path=str(output_path),
            sources=sources,
            title=request.title
        )
        
        return {
            "success": True,
            "file_path": result_path,
            "filename": filename,
            "download_url": f"/api/workspace/{workspace_id}/files/{filename}"
        }
    except ImportError as e:
        raise HTTPException(
            status_code=500, 
            detail="python-docx not installed. Run: pip install python-docx"
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/workspace/{workspace_id}/synthesis/export")
async def export_synthesis_to_docx(workspace_id: str, content: str, title: str = "Literature Synthesis"):
    """Export a synthesis directly to DOCX."""
    from services.document_exporter import document_exporter
    
    try:
        result_path = document_exporter.export_synthesis_to_docx(
            workspace_id=workspace_id,
            synthesis_content=content
        )
        
        return {
            "success": True,
            "file_path": result_path,
            "message": "Synthesis exported to DOCX with clickable citations"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# LITERATURE SYNTHESIS ENDPOINTS
# ============================================================================

class SynthesisRequest(BaseModel):
    """Request for literature synthesis."""
    topic: str
    output_format: str = "markdown"  # markdown or docx


@app.post("/api/workspace/{workspace_id}/synthesize")
async def synthesize_literature(workspace_id: str, request: SynthesisRequest):
    """
    Generate a literature synthesis from all collected sources.
    
    Reads PDFs, abstracts, texts, JSONs and generates a well-cited report.
    Returns SSE stream with synthesized content.
    """
    from services.literature_synthesis import literature_synthesis_service
    
    async def synthesis_generator():
        try:
            # Send start event
            yield {
                "event": "start",
                "data": json.dumps({"topic": request.topic, "workspace_id": workspace_id})
            }
            
            # Stream synthesis
            async for chunk in literature_synthesis_service.synthesize_literature(
                workspace_id=workspace_id,
                topic=request.topic,
                output_format=request.output_format
            ):
                yield {
                    "event": "content",
                    "data": json.dumps({"text": chunk})
                }
            
            # Send complete event
            yield {
                "event": "complete",
                "data": json.dumps({"status": "success"})
            }
            
        except Exception as e:
            yield {
                "event": "error",
                "data": json.dumps({"error": str(e)})
            }
    
    return EventSourceResponse(synthesis_generator())


@app.get("/api/workspace/{workspace_id}/synthesize/sources")
async def get_synthesis_sources(workspace_id: str):
    """Preview what sources will be used for synthesis."""
    try:
        from services.literature_synthesis import literature_synthesis_service
        
        sources = await literature_synthesis_service.gather_source_content(workspace_id)
        
        return {
            "workspace_id": workspace_id,
            "total_sources": len(sources),
            "sources": [
                {
                    "id": s.get("id"),
                    "title": s.get("title"),
                    "authors": s.get("authors", [])[:2],
                    "year": s.get("year"),
                    "source_type": s.get("source_type"),
                    "content_length": len(s.get("content", "")),
                    "citation_key": s.get("citation_key")
                }
                for s in sources
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# PERSISTENT JOBS ENDPOINTS
# ============================================================================

class CreateJobRequest(BaseModel):
    """Request to create a new background job."""
    message: str
    mentioned_agents: Optional[List[str]] = None
    files: Optional[List[str]] = None



@app.post("/api/workspace/{workspace_id}/jobs")
async def create_job(workspace_id: str, request: CreateJobRequest):
    """
    Create a new persistent background job.
    
    The job runs independently of the frontend connection.
    Returns immediately with job_id - use /stream to follow progress.
    """
    try:
        from services.job_manager import job_manager
        
        # Create the job
        job = await job_manager.create_job(
            workspace_id=workspace_id,
            message=request.message,
            mentioned_agents=request.mentioned_agents,
            files=request.files
        )
        
        # Start job processing in background
        from services.job_processor import process_job
        await job_manager.start_job(job, process_job)
        
        return {
            "status": "created",
            "job_id": job.job_id,
            "workspace_id": workspace_id,
            "stream_url": f"/api/workspace/{workspace_id}/jobs/{job.job_id}/stream"
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/workspace/{workspace_id}/jobs")
async def list_jobs(
    workspace_id: str,
    status: Optional[str] = None,
    limit: int = 20
):
    """List jobs for a workspace."""
    try:
        from services.job_manager import job_manager, JobStatus
        
        status_filter = JobStatus(status) if status else None
        jobs = job_manager.list_jobs(workspace_id, status=status_filter, limit=limit)
        
        return {
            "workspace_id": workspace_id,
            "total": len(jobs),
            "jobs": [job.to_dict() for job in jobs]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/workspace/{workspace_id}/jobs/active")
async def get_active_jobs(workspace_id: str):
    """Get currently running or paused jobs."""
    try:
        from services.job_manager import job_manager, JobStatus
        
        jobs = job_manager.list_jobs(workspace_id)
        active_jobs = [
            job for job in jobs 
            if job.status in [JobStatus.RUNNING, JobStatus.PAUSED, JobStatus.PENDING]
        ]
        
        return {
            "workspace_id": workspace_id,
            "active_count": len(active_jobs),
            "jobs": [job.to_dict() for job in active_jobs]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/workspace/{workspace_id}/jobs/{job_id}")
async def get_job(workspace_id: str, job_id: str):
    """Get job status and details."""
    try:
        from services.job_manager import job_manager
        
        job = job_manager.get_job(workspace_id, job_id)
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        
        return {
            "job": job.to_dict(),
            "is_active": job_manager.is_job_active(job_id)
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/workspace/{workspace_id}/jobs/{job_id}/stream")
async def stream_job(workspace_id: str, job_id: str):
    """
    SSE stream for job events.
    
    Can be reconnected at any time - will continue receiving events.
    Events: progress, step, content, log, completed, error, paused, resumed, cancelled
    """
    from services.job_manager import job_manager
    
    job = job_manager.get_job(workspace_id, job_id)
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")
    
    async def event_generator():
        # Send current job state first
        yield {
            "event": "state",
            "data": json.dumps(job.to_dict())
        }
        
        # Stream events
        async for event in job_manager.get_event_stream(job_id):
            yield {
                "event": event["type"],
                "data": json.dumps(event["data"])
            }
    
    return EventSourceResponse(event_generator())


@app.post("/api/workspace/{workspace_id}/jobs/{job_id}/pause")
async def pause_job(workspace_id: str, job_id: str):
    """Pause a running job."""
    try:
        from services.job_manager import job_manager
        
        success = await job_manager.pause_job(workspace_id, job_id)
        if not success:
            raise HTTPException(status_code=400, detail="Cannot pause job - not running")
        
        return {"status": "paused", "job_id": job_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/workspace/{workspace_id}/jobs/{job_id}/resume")
async def resume_job(workspace_id: str, job_id: str):
    """Resume a paused job."""
    try:
        from services.job_manager import job_manager
        
        success = await job_manager.resume_job(workspace_id, job_id)
        if not success:
            raise HTTPException(status_code=400, detail="Cannot resume job - not paused")
        
        return {"status": "resumed", "job_id": job_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/workspace/{workspace_id}/jobs/{job_id}/cancel")
async def cancel_job(workspace_id: str, job_id: str):
    """Cancel a running or paused job."""
    try:
        from services.job_manager import job_manager
        
        success = await job_manager.cancel_job(workspace_id, job_id)
        if not success:
            raise HTTPException(status_code=400, detail="Cannot cancel job - already completed or cancelled")
        
        return {"status": "cancelled", "job_id": job_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# PROJECT ENDPOINTS
# ============================================================================

class CreateProjectRequest(BaseModel):
    name: str
    project_type: Optional[str] = "default"
    folder_id: Optional[str] = None


@app.post("/api/workspace/{workspace_id}/projects")
async def create_project(workspace_id: str, request: CreateProjectRequest):
    """Create a new project (same as folder for now)."""
    try:
        workspace_path = WORKSPACES_DIR / workspace_id
        project_path = workspace_path / request.name
        
        project_path.mkdir(parents=True, exist_ok=True)
        
        return {
            "status": "success",
            "path": str(project_path.relative_to(workspace_path)),
            "name": request.name,
            "type": "project"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/workspace/{workspace_id}/folders")
async def create_folder(workspace_id: str, request: CreateFolderRequest):
    """Create a new folder."""
    try:
        workspace_path = WORKSPACES_DIR / workspace_id
        folder_path = workspace_path / (request.path or request.name)
        
        folder_path.mkdir(parents=True, exist_ok=True)
        
        return {
            "status": "success",
            "path": str(folder_path.relative_to(workspace_path)),
            "name": folder_path.name
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# IMAGE DOWNLOAD ENDPOINTS
# ============================================================================

class DownloadImageRequest(BaseModel):
    image_url: str
    filename: Optional[str] = None

class BatchDownloadImagesRequest(BaseModel):
    images: List[Dict[str, str]]  # List of {url, filename}

@app.post("/api/workspace/{workspace_id}/download-image")
async def download_image(workspace_id: str, request: DownloadImageRequest):
    """Download a single image from URL and save to workspace."""
    try:
        import httpx
        from pathlib import Path
        
        workspace_path = WORKSPACES_DIR / workspace_id
        images_dir = workspace_path / "images"
        images_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate filename if not provided
        if not request.filename:
            # Extract filename from URL or generate one
            url_path = request.image_url.split('/')[-1].split('?')[0]
            if url_path and '.' in url_path:
                request.filename = url_path
            else:
                request.filename = f"image_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jpg"
        
        # Ensure filename has extension
        if '.' not in request.filename:
            request.filename += ".jpg"
        
        file_path = images_dir / request.filename
        
        # Download image
        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            response = await client.get(request.image_url)
            response.raise_for_status()
            
            # Save to workspace
            file_path.write_bytes(response.content)
        
        relative_path = str(file_path.relative_to(workspace_path))
        
        return {
            "status": "success",
            "path": relative_path,
            "filename": request.filename,
            "size": len(response.content)
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Failed to download image: {str(e)}")

@app.post("/api/workspace/{workspace_id}/batch-download-images")
async def batch_download_images(workspace_id: str, request: BatchDownloadImagesRequest):
    """Download multiple images and save to workspace."""
    try:
        import httpx
        from pathlib import Path
        
        workspace_path = WORKSPACES_DIR / workspace_id
        images_dir = workspace_path / "images"
        images_dir.mkdir(parents=True, exist_ok=True)
        
        success_count = 0
        failed_count = 0
        results = []
        
        async with httpx.AsyncClient(timeout=30.0, follow_redirects=True) as client:
            for img_data in request.images:
                try:
                    image_url = img_data.get("url") or img_data.get("image_url")
                    filename = img_data.get("filename")
                    
                    if not image_url:
                        failed_count += 1
                        results.append({"url": image_url, "status": "failed", "error": "No URL provided"})
                        continue
                    
                    # Generate filename if not provided
                    if not filename:
                        url_path = image_url.split('/')[-1].split('?')[0]
                        if url_path and '.' in url_path:
                            filename = url_path
                        else:
                            filename = f"image_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}.jpg"
                    
                    # Ensure filename has extension
                    if '.' not in filename:
                        filename += ".jpg"
                    
                    file_path = images_dir / filename
                    
                    # Download image
                    response = await client.get(image_url)
                    response.raise_for_status()
                    
                    # Save to workspace
                    file_path.write_bytes(response.content)
                    
                    relative_path = str(file_path.relative_to(workspace_path))
                    success_count += 1
                    results.append({
                        "url": image_url,
                        "status": "success",
                        "path": relative_path,
                        "filename": filename
                    })
                    
                except Exception as e:
                    failed_count += 1
                    results.append({
                        "url": img_data.get("url", "unknown"),
                        "status": "failed",
                        "error": str(e)
                    })
        
        return {
            "status": "completed",
            "total": len(request.images),
            "success": success_count,
            "failed": failed_count,
            "results": results
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Failed to batch download images: {str(e)}")

@app.post("/api/workspace/{workspace_id}/rename")
async def rename_item(workspace_id: str, request: RenameRequest):
    """Rename a file or folder."""
    try:
        workspace_path = WORKSPACES_DIR / workspace_id
        old_path = workspace_path / request.old_path
        new_path = workspace_path / request.new_path
        
        if not old_path.exists():
            raise HTTPException(status_code=404, detail="Item not found")
        
        new_path.parent.mkdir(parents=True, exist_ok=True)
        old_path.rename(new_path)
        
        return {"status": "success", "old_path": request.old_path, "new_path": request.new_path}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/workspace/{workspace_id}/files/{file_path:path}")
async def delete_file(workspace_id: str, file_path: str):
    """Delete a file or folder."""
    try:
        workspace_path = WORKSPACES_DIR / workspace_id / file_path
        
        if not workspace_path.exists():
            raise HTTPException(status_code=404, detail="Item not found")
        
        if workspace_path.is_file():
            workspace_path.unlink()
        else:
            import shutil
            shutil.rmtree(workspace_path)
        
        return {"status": "success", "message": f"Deleted {file_path}"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

class BatchDeleteRequest(BaseModel):
    paths: List[str]

class BatchDownloadRequest(BaseModel):
    paths: List[str]

class ZipRequest(BaseModel):
    paths: List[str]

@app.post("/api/workspace/{workspace_id}/batch-delete")
async def batch_delete_files(workspace_id: str, request: BatchDeleteRequest):
    """Delete multiple files/folders."""
    try:
        workspace_path = WORKSPACES_DIR / workspace_id
        deleted = []
        failed = []
        
        for file_path in request.paths:
            try:
                target_path = workspace_path / file_path
                if target_path.exists():
                    if target_path.is_file():
                        target_path.unlink()
                    else:
                        import shutil
                        shutil.rmtree(target_path)
                    deleted.append(file_path)
                else:
                    failed.append({"path": file_path, "error": "Not found"})
            except Exception as e:
                failed.append({"path": file_path, "error": str(e)})
        
        return {
            "status": "success",
            "deleted": deleted,
            "failed": failed,
            "total": len(request.paths),
            "success_count": len(deleted)
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/workspace/{workspace_id}/batch-download")
async def batch_download_files(workspace_id: str, request: BatchDownloadRequest):
    """Download multiple files as a zip."""
    try:
        import zipfile
        import io
        from pathlib import Path
        
        workspace_path = WORKSPACES_DIR / workspace_id
        
        # Create zip in memory
        zip_buffer = io.BytesIO()
        
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            for file_path in request.paths:
                target_path = workspace_path / file_path
                if target_path.exists():
                    if target_path.is_file():
                        zip_file.write(target_path, file_path)
                    else:
                        # Add folder recursively
                        for item in target_path.rglob('*'):
                            if item.is_file():
                                arcname = item.relative_to(workspace_path)
                                zip_file.write(item, str(arcname))
        
        zip_buffer.seek(0)
        
        return Response(
            content=zip_buffer.getvalue(),
            media_type="application/zip",
            headers={
                "Content-Disposition": f"attachment; filename=files-{workspace_id}-{datetime.now().strftime('%Y%m%d-%H%M%S')}.zip"
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/workspace/{workspace_id}/zip")
async def zip_files(workspace_id: str, request: ZipRequest):
    """Create a zip file from selected files/folders."""
    try:
        import zipfile
        import io
        from pathlib import Path
        
        workspace_path = WORKSPACES_DIR / workspace_id
        
        # Create zip in memory
        zip_buffer = io.BytesIO()
        
        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
            for file_path in request.paths:
                target_path = workspace_path / file_path
                if target_path.exists():
                    if target_path.is_file():
                        zip_file.write(target_path, file_path)
                    else:
                        # Add folder recursively
                        for item in target_path.rglob('*'):
                            if item.is_file():
                                arcname = item.relative_to(workspace_path)
                                zip_file.write(item, str(arcname))
        
        zip_buffer.seek(0)
        
        return Response(
            content=zip_buffer.getvalue(),
            media_type="application/zip",
            headers={
                "Content-Disposition": f"attachment; filename=workspace-{workspace_id}-{datetime.now().strftime('%Y%m%d-%H%M%S')}.zip"
            }
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# DOCUMENT MANAGEMENT & CHAT
# ============================================================================

from services.document_service import DocumentService

@app.post("/api/workspace/{workspace_id}/documents/upload")
async def upload_document(
    workspace_id: str,
    file: UploadFile = File(...)
):
    """Upload and parse a document (PDF, DOCX, images, text)."""
    try:
        from pathlib import Path
        import tempfile
        import shutil
        
        # Save uploaded file temporarily
        file_ext = Path(file.filename).suffix.lower().lstrip('.')
        allowed_extensions = ['pdf', 'docx', 'txt', 'png', 'jpg', 'jpeg', 'gif', 'webp']
        
        if file_ext not in allowed_extensions:
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported file type. Allowed: {', '.join(allowed_extensions)}"
            )
        
        # Save to temp file
        with tempfile.NamedTemporaryFile(delete=False, suffix=f".{file_ext}") as tmp_file:
            shutil.copyfileobj(file.file, tmp_file)
            tmp_path = Path(tmp_file.name)
        
        try:
            # Initialize document service
            doc_service = DocumentService(workspace_id=workspace_id)
            
            # Upload and parse
            doc_metadata = await doc_service.upload_document(
                file_path=tmp_path,
                filename=file.filename,
                file_type=file_ext
            )
            
            return {
                "status": "success",
                "document": doc_metadata
            }
        finally:
            # Clean up temp file
            if tmp_path.exists():
                tmp_path.unlink()
                
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/workspace/{workspace_id}/documents")
async def list_documents(workspace_id: str):
    """List all uploaded documents."""
    try:
        doc_service = DocumentService(workspace_id=workspace_id)
        documents = doc_service.list_documents()
        return {"status": "success", "documents": documents}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/workspace/{workspace_id}/documents/{doc_id}")
async def delete_document(workspace_id: str, doc_id: str):
    """Delete a document."""
    try:
        doc_service = DocumentService(workspace_id=workspace_id)
        success = doc_service.delete_document(doc_id)
        if not success:
            raise HTTPException(status_code=404, detail="Document not found")
        return {"status": "success", "message": "Document deleted"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

class DocumentChatRequest(BaseModel):
    message: str
    doc_ids: Optional[List[str]] = None  # If None, search all documents
    session_id: Optional[str] = "default"

@app.post("/api/workspace/{workspace_id}/documents/chat")
async def chat_with_documents(workspace_id: str, request: DocumentChatRequest):
    """Chat with uploaded documents using RAG."""
    try:
        from services.document_service import DocumentService
        from services.deepseek_direct import deepseek_direct_service
        
        doc_service = DocumentService(workspace_id=workspace_id)
        
        # Search relevant chunks
        chunks = doc_service.search_chunks(
            query=request.message,
            doc_ids=request.doc_ids,
            top_k=5
        )
        
        if not chunks:
            return {
                "response": "I couldn't find relevant information in the uploaded documents. Please try rephrasing your question or upload more documents.",
                "sources": [],
                "citations": []
            }
        
        # Build context with citations
        context_parts = []
        citations = []
        
        for i, chunk in enumerate(chunks, 1):
            doc_name = chunk["doc_name"]
            page = chunk["page"]
            text = chunk["text"][:500]  # Limit chunk size
            
            context_parts.append(f"[Document {i}: {doc_name}, Page {page}]\n{text}")
            citations.append({
                "document": doc_name,
                "page": page,
                "text": text[:200] + "..." if len(text) > 200 else text
            })
        
        context = "\n\n".join(context_parts)
        
        # Generate response with citations
        prompt = f"""You are a helpful assistant that answers questions based on uploaded documents.

User Question: {request.message}

Relevant Document Excerpts:
{context}

Instructions:
1. Answer the question using ONLY the information from the documents above
2. When referencing information, cite the document name and page number like: (Document Name, Page X)
3. If the documents don't contain enough information, say so clearly
4. Be accurate and cite sources for all claims

Answer:"""
        
        response = await deepseek_direct_service.generate_content(
            prompt=prompt,
            system_prompt="You are a document analysis assistant. Always cite your sources with document names and page numbers.",
            temperature=0.3,
            max_tokens=2000,
            use_reasoning=False
        )
        
        return {
            "response": response,
            "sources": [{"doc": c["doc_name"], "page": c["page"]} for c in chunks],
            "citations": citations
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# CUSTOM AGENT MANAGEMENT
# ============================================================================

@app.post("/api/agents/upload")
async def upload_custom_agent(file: UploadFile = File(...)):
    """Upload a custom agent configuration as JSON."""
    try:
        import json
        
        # Read and validate JSON
        content = await file.read()
        try:
            agent_config = json.loads(content.decode('utf-8'))
        except json.JSONDecodeError as e:
            raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")
        
        # Validate required fields
        required_fields = ["name", "description", "tools", "system_prompt"]
        for field in required_fields:
            if field not in agent_config:
                raise HTTPException(status_code=400, detail=f"Missing required field: {field}")
        
        # Save agent config
        agents_dir = Path("agents/custom")
        agents_dir.mkdir(parents=True, exist_ok=True)
        
        agent_id = agent_config.get("id") or hashlib.md5(agent_config["name"].encode()).hexdigest()[:16]
        agent_file = agents_dir / f"{agent_id}.json"
        
        agent_config["id"] = agent_id
        agent_config["uploaded_at"] = datetime.now().isoformat()
        agent_config["type"] = "custom"
        
        agent_file.write_text(json.dumps(agent_config, indent=2))
        
        return {
            "status": "success",
            "agent_id": agent_id,
            "message": f"Agent '{agent_config['name']}' uploaded successfully"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/agents/custom")
async def list_custom_agents():
    """List all custom uploaded agents."""
    try:
        agents_dir = Path("agents/custom")
        agents = []
        
        if agents_dir.exists():
            for agent_file in agents_dir.glob("*.json"):
                try:
                    agent_config = json.loads(agent_file.read_text())
                    agents.append({
                        "id": agent_config.get("id"),
                        "name": agent_config.get("name"),
                        "description": agent_config.get("description"),
                        "uploaded_at": agent_config.get("uploaded_at"),
                        "tools": agent_config.get("tools", [])
                    })
                except:
                    continue
        
        return {"status": "success", "agents": agents}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.delete("/api/agents/custom/{agent_id}")
async def delete_custom_agent(agent_id: str):
    """Delete a custom agent."""
    try:
        agent_file = Path(f"agents/custom/{agent_id}.json")
        if not agent_file.exists():
            raise HTTPException(status_code=404, detail="Agent not found")
        
        agent_file.unlink()
        return {"status": "success", "message": "Agent deleted"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ============================================================================
# CHAT ENDPOINT
# ============================================================================

def is_simple_greeting(message: str) -> bool:
    """Detect simple greetings that don't need planning."""
    message_lower = message.lower().strip()
    greetings = ["hi", "hello", "hey", "greetings", "good morning", "good afternoon", "good evening", "howdy"]
    return message_lower in greetings or message_lower in [g + "!" for g in greetings] or message_lower in [g + "." for g in greetings]

def is_simple_question(message: str) -> bool:
    """
    Detect simple questions/requests that should get direct AI response, NOT planning.
    
    Examples that should return True (direct response):
    - "write Einstein's equation"
    - "what is E=mc2?"  
    - "explain photosynthesis"
    - "give me an example of recursion"
    - "show me the quadratic formula"
    
    Examples that should return False (needs planning):
    - "write an essay about Einstein"
    - "write a 500 word document"
    - "create a report on climate change"
    """
    message_lower = message.lower().strip()
    word_count = len(message.split())
    
    # Very short messages (< 12 words) without document keywords are likely simple
    if word_count < 12:
        # Check for document generation indicators
        document_indicators = [
            "essay", "document", "report", "paper", "article",
            "chapter", "book", "thesis", "dissertation",
            "word", "page", "paragraph",  # word count requests
        ]
        has_document_keyword = any(ind in message_lower for ind in document_indicators)
        
        # Check for explicit length requests like "500 word"
        import re
        has_length_request = bool(re.search(r'\d+\s*(word|page|paragraph)', message_lower))
        
        if not has_document_keyword and not has_length_request:
            # Simple question patterns
            simple_patterns = [
                r"^(what|how|when|where|why|who|which|can you|could you|please|tell me|show me|give me|explain|define|describe)",
                r"(equation|formula|definition|example|meaning|difference|between|list|the \w+ of)",
                r"(write|show|give|tell).*\b(the|a|an|this|that|one)\b",  # "write the equation"
            ]
            if any(re.search(pattern, message_lower) for pattern in simple_patterns):
                return True
            
            # Short imperative without document keywords
            if word_count < 8:
                return True
    
    return False

def is_image_generation_request(message: str) -> bool:
    """
    Detect if user wants to GENERATE (create AI) an image.
    
    PRIORITY ORDER:
    1. Image search is default for "image of X", "picture of X"
    2. Image generation ONLY for:
       - Explicit: "generate image", "create image", "draw me"
       - Diagrams, frameworks, illustrations, charts (things that need AI creation)
    """
    import re
    message_lower = message.lower()
    
    # Don't trigger if user says "search" or "find"
    if any(s in message_lower for s in ["search", "find", "look for", "get me a photo"]):
        return False
    
    # Generic "image of X" or "picture of X" should use SEARCH, not generate
    # Only explicit generation commands trigger this
    
    # EXPLICIT generation keywords
    explicit_generate = [
        "generate image", "create image", "make image", 
        "generate a picture", "create a picture", "make a picture",
        "generate an image", "create an image", "make an image",
        "draw me", "generate me", "create me",
        "generate illustration", "create illustration",
        "generate diagram", "create diagram", "make diagram",
    ]
    
    if any(keyword in message_lower for keyword in explicit_generate):
        return True
    
    # GENERATION-ONLY content (things that can't be searched, must be created)
    generation_content = [
        "diagram", "flowchart", "framework", "schematic",
        "infographic", "chart", "visualization", "concept map",
        "theoretical model", "research model", "study framework",
        "illustration of concept", "visualize the", "architectural diagram"
    ]
    
    if any(content in message_lower for content in generation_content):
        return True
    
    return False

def is_paper_search_request(message: str) -> bool:
    """
    Detect if user wants to SEARCH for academic papers/sources.
    
    Examples that should return True:
    - "search for papers on machine learning"
    - "find papers about climate change"
    - "look for academic articles on AI"
    - "search literature on healthcare"
    - "find research on deep learning"
    """
    message_lower = message.lower().strip()
    
    # Search trigger phrases
    search_triggers = [
        "search for papers", "search papers", "find papers",
        "search for articles", "search articles", "find articles",
        "search literature", "find literature", "search for literature",
        "search for research", "find research", "look for papers",
        "look for articles", "look for research", "search academic",
        "find academic", "search for sources", "find sources",
        "paper search", "literature search", "research search"
    ]
    
    return any(trigger in message_lower for trigger in search_triggers)

def is_research_synthesis_request(message: str) -> tuple:
    """
    Intelligently detect if user wants a RESEARCH SYNTHESIS - search + analyze + write.
    
    This understands natural language variations like:
    - "search papers and write synthesis on climate change"
    - "find research on AI and synthesize the findings"
    - "literature review on machine learning"
    - "give me a synthesis of papers about healthcare"
    - "research wars in sudan and write a report"
    
    Returns: (is_synthesis: bool, topic: str, style: str)
    """
    import re
    message_lower = message.lower().strip()
    
    # Pattern 1: Explicit "search...and...synthesis/review/report"
    explicit_patterns = [
        r"(search|find|look\s+for).*?(papers?|research|literature|articles?).*?and.*?(write|create|generate|make|give).*?(synthesis|review|report|summary)",
        r"(search|find).*?and.*?(synthesize|summarize|analyze)",
        r"(literature\s+review|research\s+synthesis|paper\s+synthesis).*?(on|about|for)",
        r"(synthesize|summarize).*?(papers?|research|literature|findings?).*?(on|about)",
        r"(give|provide|create|write).*?synthesis.*?(on|about|of)",
    ]
    
    for pattern in explicit_patterns:
        if re.search(pattern, message_lower):
            # Extract topic - look for "on X", "about X", or just the noun phrase
            topic_match = re.search(r'(?:on|about|for|of)\s+(.+?)(?:\s+and\s+|\s+with\s+|$)', message_lower)
            if topic_match:
                topic = topic_match.group(1).strip()
                # Clean up topic
                topic = re.sub(r'\s+(and|then|after|with).*', '', topic)
                return (True, topic, "academic")
            # Fallback - extract from middle of sentence
            topic = re.sub(r'^(search|find|look\s+for|give|provide|create|write|make)\s+', '', message_lower)
            topic = re.sub(r'\s+(and|then|after).*', '', topic)
            topic = re.sub(r'(papers?|research|literature|articles?|synthesis|review)\s*(on|about)?\s*', '', topic)
            return (True, topic.strip() or message, "academic")
    
    # Pattern 2: Implicit - mentions research topic with action words suggesting report
    implicit_patterns = [
        r"(research|study|investigate|explore)\s+.+?\s+and\s+(write|create|report)",
        r"(write|create).*?(research|literature)\s+(report|review|paper)\s+(on|about)",
    ]
    
    for pattern in implicit_patterns:
        match = re.search(pattern, message_lower)
        if match:
            topic_match = re.search(r'(?:on|about)\s+(.+?)(?:\s+and\s+|$)', message_lower)
            topic = topic_match.group(1) if topic_match else message
            return (True, topic.strip(), "research")
    
    # Pattern 3: Context-aware - "papers on X and synthesis" without explicit order
    if ("papers" in message_lower or "research" in message_lower or "literature" in message_lower):
        if ("synthesis" in message_lower or "synthesize" in message_lower or "review" in message_lower):
            topic_match = re.search(r'(?:on|about)\s+(.+?)(?:\s+and\s+|$)', message_lower)
            topic = topic_match.group(1) if topic_match else message
            return (True, topic.strip(), "combined")
    
    return (False, "", "")

def is_pdf_action_request(message: str) -> tuple:
    """
    Detect if user wants to perform an action on a PDF file.
    
    Examples that should return True:
    - "summarize the pdf"
    - "read the climate change pdf"
    - "analyze this pdf"
    - "what's in the uploaded pdf"
    - "summarize Climate_Report.pdf"
    
    Returns:
        (is_pdf_action: bool, action_type: str, pdf_name: str|None)
    """
    import re
    message_lower = message.lower().strip()
    
    # Action keywords
    action_keywords = [
        "summarize", "summarise", "read", "analyze", "analyse", 
        "extract", "what's in", "what is in", "tell me about",
        "explain", "overview", "key points", "main points"
    ]
    
    # Check if it's a PDF action
    has_action = any(action in message_lower for action in action_keywords)
    has_pdf_mention = "pdf" in message_lower or ".pdf" in message_lower
    
    if has_action and has_pdf_mention:
        # Try to extract PDF filename from message
        pdf_match = re.search(r'(\S+\.pdf)', message, re.IGNORECASE)
        pdf_name = pdf_match.group(1) if pdf_match else None
        
        # Determine action type
        if any(a in message_lower for a in ["summarize", "summarise", "summary"]):
            action_type = "summarize"
        elif any(a in message_lower for a in ["extract", "key points", "main points"]):
            action_type = "extract"
        elif any(a in message_lower for a in ["analyze", "analyse"]):
            action_type = "analyze"
        else:
            action_type = "read"
        
        return (True, action_type, pdf_name)
    
    return (False, None, None)

@app.post("/api/chat/message")
async def chat_message(request: ChatMessageRequest):
    """Handle chat messages with planner and tool execution."""
    # Generate job_id IMMEDIATELY at the start - before any processing
    # This ensures frontend can always connect to stream even if we error
    job_id = str(uuid.uuid4())
    
    # Ensure request has required fields with defaults
    if not hasattr(request, 'message') or not request.message:
        return {
            "response": "Please provide a message.",
            "reasoning": "",
            "plan": [],
            "tool_results": {},
            "job_id": job_id
        }
    
    # Set defaults if not provided
    if not hasattr(request, 'session_id') or not request.session_id:
        request.session_id = "default"
    if not hasattr(request, 'workspace_id') or not request.workspace_id:
        request.workspace_id = "default"
    if not hasattr(request, 'user_id') or not request.user_id:
        request.user_id = "default"
    
    try:
        message_lower = request.message.lower().strip()
        
        # =====================================================================
        # FAST INTELLIGENT ROUTER - Speed is priority!
        # Classifies and routes requests in <10ms wherever possible
        # =====================================================================
        
        def fast_classify(msg: str) -> dict:
            """Ultra-fast request classification using simple pattern matching."""
            msg_lower = msg.lower().strip()
            words = msg_lower.split()
            word_count = len(words)
            
            # GREETINGS - instant response
            if word_count < 5 and any(g in msg_lower for g in ["hi", "hello", "hey", "greetings", "sup", "yo"]):
                return {"type": "greeting", "route": "instant"}
            
            # SIMPLE QUESTIONS - direct LLM streaming (fastest)
            question_starters = ["what", "who", "when", "where", "why", "how", "is", "are", "can", "do", "does", "will", "would", "should", "could", "explain", "define", "describe"]
            if any(msg_lower.startswith(q) for q in question_starters) and word_count < 30:
                # Check if it needs tools
                needs_tools = any(t in msg_lower for t in ["search", "find", "generate", "create", "make", "write document", "write essay", "file"])
                if not needs_tools:
                    return {"type": "question", "route": "direct_llm"}
            
            # UNSUPPORTED REQUESTS - respond gracefully
            unsupported_keywords = {
                "video": "Video generation is not yet supported. I can help with text, images, and research.",
                "audio": "Audio generation is not yet supported. I can help with text, images, and research.",
                "music": "Music generation is not yet supported. I can help with text, images, and research.",
                "voice": "Voice synthesis is not yet supported. I can help with text, images, and research.",
                "song": "Song creation is not yet supported. I can help with text, images, and research.",
            }
            for keyword, message in unsupported_keywords.items():
                if keyword in msg_lower and any(a in msg_lower for a in ["make", "create", "generate", "produce"]):
                    return {"type": "unsupported", "route": "graceful", "message": message}
            
            # IMAGE SEARCH - PRIORITY over generation!
            # Most "image of X" requests should search, not generate
            if ("image" in msg_lower or "picture" in msg_lower or "photo" in msg_lower):
                # Check if search is explicitly requested
                if any(s in msg_lower for s in ["search", "find", "look for", "get me"]):
                    return {"type": "image_search", "route": "tool"}
                # Default "image of X" / "picture of X" = search (not generate)
                if any(p in msg_lower for p in ["image of", "picture of", "photo of", "images of", "pictures of"]):
                    return {"type": "image_search", "route": "tool"}
            
            # IMAGE GENERATION - ONLY for specific cases that NEED AI generation:
            # - Diagrams, frameworks, illustrations, charts, conceptual art
            # - When user explicitly says "generate", "create", "draw"
            generation_keywords = ["diagram", "framework", "illustration", "chart", "flowchart", 
                                   "infographic", "concept", "visualize", "schematic", "model diagram",
                                   "study framework", "research model", "theoretical framework"]
            explicit_generate = any(g in msg_lower for g in ["generate image", "create image", "draw me", "make me an image"])
            needs_generation = any(k in msg_lower for k in generation_keywords)
            
            if explicit_generate or needs_generation:
                return {"type": "image_generate", "route": "tool"}
            
            # PAPER/ACADEMIC SEARCH
            if any(p in msg_lower for p in ["search paper", "find paper", "search research", "academic", "search literature"]):
                if "synthesis" in msg_lower or "synthesize" in msg_lower or "review" in msg_lower:
                    return {"type": "research_synthesis", "route": "pipeline"}
                return {"type": "paper_search", "route": "tool"}
            
            # WEB SEARCH
            if any(p in msg_lower for p in ["search", "look up", "look for", "find out"]) and word_count < 15:
                return {"type": "web_search", "route": "tool"}
            
            # ESSAY/DOCUMENT WRITING - needs worker
            if any(p in msg_lower for p in ["write essay", "write document", "write report", "write paper", "essay about", "essay on"]):
                return {"type": "essay", "route": "worker"}
            
            # SHORT CASUAL CHAT - direct LLM
            if word_count < 20 and not any(t in msg_lower for t in ["file", "save", "create", "generate", "search", "document"]):
                return {"type": "chat", "route": "direct_llm"}
            
            # DEFAULT - let LLM handle with possible tool use
            return {"type": "general", "route": "orchestrator"}
        
        # =====================================================
        # INTELLIGENT INTENT SYSTEM - True AI Understanding
        # =====================================================
        from services.intelligent_intent import intelligent_intent, IntentType, RouteType
        
        # Use intelligent intent system (fast patterns + LLM for ambiguous cases)
        intent_result = await intelligent_intent.understand(request.message)
        print(f"ðŸ§  Intelligent Intent: {intent_result.intent.value} -> {intent_result.route.value} (confidence: {intent_result.confidence:.2f})")
        print(f"   Reasoning: {intent_result.reasoning}")
        
        # Convert to route_info format for compatibility
        route_info = {
            "type": intent_result.intent.value,
            "route": intent_result.route.value,
            "params": intent_result.params,
            "message": intent_result.message,
            "confidence": intent_result.confidence
        }
        
        # ROUTE 1: Unsupported requests - respond gracefully and fast
        if route_info["route"] == "graceful":
            await events.connect()
            response_text = route_info.get("message", "This feature is not yet supported.")
            await events.publish(job_id, "response_chunk", {
                "chunk": response_text,
                "accumulated": response_text
            }, session_id=request.session_id)
            return {
                "response": response_text,
                "reasoning": "",
                "plan": [],
                "job_id": job_id
            }
        
        # ROUTE 2: Direct LLM streaming - NO tools, maximum speed
        if route_info["route"] == "direct_llm":
            from services.deepseek_direct import deepseek_direct_service
            
            async def stream_direct_response():
                """Stream LLM response directly - fastest path."""
                import httpx
                
                try:
                    if not deepseek_direct_service.api_key:
                        yield {"event": "content", "data": json.dumps({"text": "âš ï¸ LLM API key not configured."})}
                        yield {"event": "done", "data": json.dumps({"status": "error"})}
                        return
                    
                    headers = {
                        "Authorization": f"Bearer {deepseek_direct_service.api_key}",
                        "Content-Type": "application/json"
                    }
                    
                    payload = {
                        "model": "deepseek-chat",
                        "messages": [
                            {"role": "system", "content": "You are a helpful, knowledgeable AI assistant. Be concise and direct. Answer questions clearly."},
                            {"role": "user", "content": request.message}
                        ],
                        "temperature": 0.7,
                        "max_tokens": 2000,
                        "stream": True
                    }
                    
                    yield {"event": "stage", "data": json.dumps({"stage": "responding", "icon": "ðŸ’¬", "message": "Generating response..."})}
                    
                    async with httpx.AsyncClient(timeout=60.0) as client:
                        async with client.stream(
                            "POST",
                            f"{deepseek_direct_service.base_url}/chat/completions",
                            headers=headers,
                            json=payload
                        ) as response:
                            response.raise_for_status()
                            async for line in response.aiter_lines():
                                if not line or line.strip() == "":
                                    continue
                                if line.startswith("data: "):
                                    line = line[6:]
                                if line.strip() == "[DONE]":
                                    break
                                
                                try:
                                    chunk_data = json.loads(line)
                                    if "choices" in chunk_data and len(chunk_data["choices"]) > 0:
                                        delta = chunk_data["choices"][0].get("delta", {})
                                        content = delta.get("content", "")
                                        if content:
                                            yield {"event": "content", "data": json.dumps({"text": content})}
                                except json.JSONDecodeError:
                                    continue
                    
                    yield {"event": "done", "data": json.dumps({"status": "success"})}
                    
                except Exception as e:
                    yield {"event": "error", "data": json.dumps({"message": str(e)})}
            
            return EventSourceResponse(stream_direct_response())
        
        # ROUTE 3: Direct IMAGE SEARCH - Uses Unsplash/Pexels/Pixabay APIs directly (no LLM!)
        if route_info["route"] == "tool_direct" and route_info["type"] == "image_search":
            from services.image_search import image_search_service  # Direct API calls, no LLM
            import asyncio
            
            # Extract search query
            query = request.message
            for prefix in ["search for", "find", "look for", "get me", "show me", "image of", "picture of", "photo of", "images of", "pictures of"]:
                query = query.lower().replace(prefix, "").strip()
            
            async def run_image_search():
                """Run image search and publish events via Redis."""
                try:
                    await events.connect()
                    
                    # Send stage_started
                    await events.publish(job_id, "stage_started", {"stage": "image_search", "message": f"ðŸ” Searching images: {query}..."}, session_id=request.session_id)
                    
                    # Call image search APIs directly
                    results = await image_search_service.search(query, limit=6)
                    
                    if results and len(results) > 0:
                        # Format response with images
                        response_parts = [f"Found {len(results)} images for '{query}':\n\n"]
                        
                        for i, img in enumerate(results[:6]):
                            title = img.get("title", f"Image {i+1}")
                            url = img.get("url") or img.get("full") or img.get("thumbnail")
                            source = img.get("source", "Unknown")
                            if url:
                                response_parts.append(f"![{title}]({url})\n*Source: {source}*\n\n")
                        
                        response_text = "".join(response_parts)
                        
                        # Send response_chunk
                        await events.publish(job_id, "response_chunk", {"chunk": response_text, "accumulated": response_text}, session_id=request.session_id)
                        
                        # Send agent_activity for preview panel
                        await events.publish(job_id, "agent_activity", {
                            "agent": "image_search",
                            "action": "completed",
                            "query": query,
                            "status": "completed",
                            "results": results[:6]
                        }, session_id=request.session_id)
                        
                        # Send stage_completed
                        await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "success"}, session_id=request.session_id)
                    else:
                        await events.publish(job_id, "response_chunk", {"chunk": f"No images found for '{query}'. Try a different search term.", "accumulated": f"No images found for '{query}'. Try a different search term."}, session_id=request.session_id)
                        await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "no_results"}, session_id=request.session_id)
                        
                except Exception as e:
                    print(f"Image search error: {e}")
                    await events.publish(job_id, "response_chunk", {"chunk": f"Error searching images: {str(e)}", "accumulated": f"Error searching images: {str(e)}"}, session_id=request.session_id)
                    await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "error"}, session_id=request.session_id)
            
            # Run in background
            asyncio.create_task(run_image_search())
            
            # Return JSON immediately
            return {"response": f"Searching images for '{query}'...", "job_id": job_id, "plan": [], "reasoning": ""}
        
        # ROUTE 4: Direct WEB SEARCH
        if route_info["route"] == "tool_direct" and route_info["type"] == "web_search":
            from services.web_search import WebSearchService
            import asyncio
            web_search = WebSearchService()
            
            query = route_info.get("params", {}).get("query") or request.message
            
            async def run_web_search():
                try:
                    await events.connect()
                    await events.publish(job_id, "stage_started", {"stage": "web_search", "message": f"ðŸŒ Searching web: {query}..."}, session_id=request.session_id)
                    
                    results = await web_search.search(query, max_results=5)
                    
                    if results and results.get("results"):
                        response_parts = [f"**Web search results for '{query}':**\n\n"]
                        for i, r in enumerate(results["results"][:5]):
                            title = r.get("title", f"Result {i+1}")
                            url = r.get("url", "")
                            snippet = r.get("content", r.get("snippet", ""))[:200]
                            response_parts.append(f"{i+1}. **[{title}]({url})**\n   {snippet}...\n\n")
                        
                        response_text = "".join(response_parts)
                        await events.publish(job_id, "response_chunk", {"chunk": response_text, "accumulated": response_text}, session_id=request.session_id)
                        await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "success"}, session_id=request.session_id)
                    else:
                        await events.publish(job_id, "response_chunk", {"chunk": f"No results found for '{query}'.", "accumulated": f"No results found for '{query}'."}, session_id=request.session_id)
                        await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "no_results"}, session_id=request.session_id)
                except Exception as e:
                    await events.publish(job_id, "response_chunk", {"chunk": f"Error: {str(e)}", "accumulated": f"Error: {str(e)}"}, session_id=request.session_id)
                    await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "error"}, session_id=request.session_id)
            
            asyncio.create_task(run_web_search())
            return {"response": f"Searching web for '{query}'...", "job_id": job_id, "plan": [], "reasoning": ""}
        
        # ROUTE 5: Direct PAPER SEARCH
        if route_info["route"] == "tool_direct" and route_info["type"] == "paper_search":
            from services.academic_search import academic_search_service
            import asyncio
            
            query = route_info.get("params", {}).get("query") or request.message
            
            async def run_paper_search():
                try:
                    await events.connect()
                    await events.publish(job_id, "stage_started", {"stage": "paper_search", "message": f"ðŸ“š Searching papers: {query}..."}, session_id=request.session_id)
                    
                    results = await academic_search_service.search(query, max_results=10)
                    
                    if results and len(results) > 0:
                        response_parts = [f"**Found {len(results)} academic papers for '{query}':**\n\n"]
                        for i, paper in enumerate(results[:10]):
                            title = paper.get("title", f"Paper {i+1}")
                            authors = ", ".join(paper.get("authors", [])[:3])
                            year = paper.get("year", "")
                            doi = paper.get("doi", "")
                            url = f"https://doi.org/{doi}" if doi else paper.get("url", "")
                            response_parts.append(f"{i+1}. **{title}** ({year})\n   *{authors}*\n   [Link]({url})\n\n")
                        
                        response_text = "".join(response_parts)
                        await events.publish(job_id, "response_chunk", {"chunk": response_text, "accumulated": response_text}, session_id=request.session_id)
                        await events.publish(job_id, "agent_activity", {"agent": "researcher", "action": "completed", "query": query, "status": "completed", "results": results[:10]}, session_id=request.session_id)
                        await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "success"}, session_id=request.session_id)
                    else:
                        await events.publish(job_id, "response_chunk", {"chunk": f"No papers found for '{query}'.", "accumulated": f"No papers found for '{query}'."}, session_id=request.session_id)
                        await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "no_results"}, session_id=request.session_id)
                except Exception as e:
                    await events.publish(job_id, "response_chunk", {"chunk": f"Error: {str(e)}", "accumulated": f"Error: {str(e)}"}, session_id=request.session_id)
                    await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "error"}, session_id=request.session_id)
            
            asyncio.create_task(run_paper_search())
            return {"response": f"Searching papers for '{query}'...", "job_id": job_id, "plan": [], "reasoning": ""}
        
        # ROUTE 6: Direct IMAGE GENERATE (for diagrams, illustrations)
        if route_info["route"] == "tool_direct" and route_info["type"] == "image_generate":
            from services.image_generation import image_generation_service
            import asyncio
            
            prompt = route_info.get("params", {}).get("prompt") or request.message
            
            async def run_image_generate():
                try:
                    await events.connect()
                    await events.publish(job_id, "stage_started", {"stage": "image_generate", "message": f"ðŸŽ¨ Generating image: {prompt[:50]}..."}, session_id=request.session_id)
                    
                    result = await image_generation_service.generate(prompt=prompt, size="1024x1024")
                    
                    if result.get("success"):
                        image_url = result.get("image_url") or result.get("url")
                        response_text = f"**Generated image:**\n\n![{prompt[:50]}]({image_url})\n\n*Prompt: {prompt}*"
                        
                        await events.publish(job_id, "response_chunk", {"chunk": response_text, "accumulated": response_text}, session_id=request.session_id)
                        await events.publish(job_id, "agent_activity", {"agent": "image_generator", "action": "completed", "query": prompt, "status": "completed", "results": [{"url": image_url}]}, session_id=request.session_id)
                        await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "success"}, session_id=request.session_id)
                    else:
                        await events.publish(job_id, "response_chunk", {"chunk": f"Image generation failed: {result.get('error', 'Unknown error')}", "accumulated": f"Image generation failed: {result.get('error', 'Unknown error')}"}, session_id=request.session_id)
                        await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "error"}, session_id=request.session_id)
                except Exception as e:
                    await events.publish(job_id, "response_chunk", {"chunk": f"Error: {str(e)}", "accumulated": f"Error: {str(e)}"}, session_id=request.session_id)
                    await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "error"}, session_id=request.session_id)
            
            asyncio.create_task(run_image_generate())
            return {"response": f"Generating image: {prompt[:50]}...", "job_id": job_id, "plan": [], "reasoning": ""}
        
        # ROUTE 7: CHAPTER GENERATION - Parallel multi-agent thesis chapter generation
        if route_info["route"] == "pipeline" and route_info["type"] in ["chapter_generate", "chapter_two_generate", "chapter_three_generate"]:
            from services.parallel_chapter_generator import parallel_chapter_generator, BACKGROUND_STYLES
            import asyncio
            
            topic = route_info.get("params", {}).get("topic") or request.message
            case_study = route_info.get("params", {}).get("case_study", "")
            background_style = route_info.get("params", {}).get("background_style", "inverted_pyramid")
            # IMPORTANT: Read generation_type from params (set by intelligent_intent.py)
            # This determines if we generate Chapter 1, 2, or 3
            chapter_type = route_info.get("params", {}).get("generation_type") or route_info.get("type", "chapter_generate")
            
            # Get objectives if provided (for Chapter Two and Three)
            objectives = route_info.get("params", {}).get("objectives")
            research_questions = route_info.get("params", {}).get("research_questions")
            
            # Validate background style
            if background_style not in BACKGROUND_STYLES:
                background_style = "inverted_pyramid"
            
            async def run_chapter_generation():
                try:
                    await events.connect()
                    
                    # FULL PROPOSAL - Generate all 3 chapters sequentially
                    print(f"ðŸ” DEBUG: chapter_type = '{chapter_type}'")
                    if chapter_type == "full_proposal_generate":
                        print("ðŸŽ¯ DEBUG: Entered full_proposal_generate route!")
                        # Initialize session_id for events
                        session_id = request.session_id # Use request's session_id
                        print(f"ðŸ” DEBUG: session_id = {session_id}")
                        
                        await events.publish(
                            job_id,
                            "response_chunk",
                            {"chunk": f"# ðŸ“š Generating Full Research Proposal (Chapters 1-3)\n\n**Topic:** {topic}\n**Case Study:** {case_study}\n\nThis will generate:\n- âœ… Chapter 1: Introduction\n- âœ… Chapter 2: Literature Review\n- âœ… Chapter 3: Research Methodology\n\nObjectives from Chapter 1 will be used throughout.\n\n---\n\n", "accumulated": ""},
                            session_id=session_id
                        )
                        
                        
                        # Step 1: Generate Chapter 1
                        print("ðŸ” DEBUG: About to generate Chapter 1...")
                        await events.publish(job_id, "log", {"message": "ðŸ“– Step 1/3: Generating Chapter 1..."}, session_id=session_id)
                        chapter1_result = await parallel_chapter_generator.generate(
                            topic=topic,
                            case_study=case_study,
                            job_id=job_id,
                            session_id=session_id
                        )
                        print(f"âœ… DEBUG: Chapter 1 complete! Length: {len(chapter1_result)} chars")
                        
                        # Step 2: Generate Chapter 2 (using objectives from Chapter 1)
                        print("ðŸ” DEBUG: About to generate Chapter 2...")
                        await events.publish(job_id, "log", {"message": "ðŸ“š Step 2/3: Generating Chapter 2 (using Chapter 1 objectives)..."}, session_id=session_id)
                        chapter2_result = await parallel_chapter_generator.generate_chapter_two(
                            topic=topic,
                            case_study=case_study,
                            job_id=job_id,
                            session_id=session_id
                        )
                        print(f"âœ… DEBUG: Chapter 2 complete! Length: {len(chapter2_result)} chars")
                        
                        # Step 3: Generate Chapter 3 (using objectives from Chapter 1)
                        print("ðŸ” DEBUG: About to generate Chapter 3...")
                        await events.publish(job_id, "log", {"message": "ðŸ”¬ Step 3/3: Generating Chapter 3 (using Chapter 1 objectives)..."}, session_id=session_id)
                        chapter3_result = await parallel_chapter_generator.generate_chapter_three(
                            topic=topic,
                            case_study=case_study,
                            job_id=job_id,
                            session_id=session_id
                        )
                        
                        # Combine all chapters
                        full_proposal = f"{chapter1_result}\n\n---\n\n{chapter2_result}\n\n---\n\n{chapter3_result}"
                        
                        # Extract and consolidate all references
                        await events.publish(job_id, "log", {"message": "ðŸ“š Consolidating references from all chapters..."}, session_id=session_id)
                        
                        import re
                        # Extract all citations in format (Author, Year) or (Author et al., Year)
                        all_citations = set()
                        
                        # Pattern to match APA citations
                        citation_pattern = r'\(([A-Z][a-zA-Z\s&,\.]+,\s*\d{4}[a-z]?)\)'
                        
                        for chapter in [chapter1_result, chapter2_result, chapter3_result]:
                            citations = re.findall(citation_pattern, chapter)
                            all_citations.update(citations)
                        
                        # Sort citations alphabetically
                        sorted_citations = sorted(list(all_citations))
                        
                        # Create References section
                        references_section = "\n\n---\n\n# References\n\n"
                        references_section += "*Note: This is a consolidated list of all citations from Chapters 1-3. Full bibliographic details should be added based on the actual sources used.*\n\n"
                        
                        for citation in sorted_citations:
                            # Extract author and year
                            parts = citation.rsplit(',', 1)
                            if len(parts) == 2:
                                author = parts[0].strip()
                                year = parts[1].strip()
                                references_section += f"- {author} ({year}). *[Full reference details to be added]*\n"
                        
                        # Add references to full proposal
                        full_proposal_with_refs = full_proposal + references_section
                        
                        # Generate appendices (study tools)
                        await events.publish(job_id, "log", {"message": "ðŸ“Ž Generating study tools appendices..."}, session_id=session_id)
                        
                        generated_files = []
                        try:
                            from services.appendix_generator import AppendixGenerator
                            from pathlib import Path
                            import glob
                            
                            # Get objectives from database
                            objectives = []
                            research_questions = []
                            try:
                                from services.thesis_session_db import ThesisSessionDB
                                db = ThesisSessionDB(session_id)
                                obj_data = db.get_objectives() or {}
                                # get_objectives returns {"general": str, "specific": list}
                                objectives = obj_data.get("specific", [])
                                if obj_data.get("general"):
                                    objectives = [obj_data["general"]] + objectives
                                print(f"ðŸ“‹ Found {len(objectives)} objectives for study tools generation")
                                
                                # Get research questions
                                try:
                                    rq_data = db.get_questions() or []
                                    research_questions = rq_data if isinstance(rq_data, list) else []
                                    print(f"ðŸ“‹ Found {len(research_questions)} research questions")
                                except:
                                    pass
                                    
                            except Exception as obj_err:
                                print(f"âš ï¸ Could not get objectives: {obj_err}")
                            
                            # Read Chapter 3 methodology content
                            methodology_content = ""
                            try:
                                workspace_dir = Path(f"/home/gemtech/Desktop/thesis/thesis_data/default")
                                chapter3_files = list(workspace_dir.glob("Chapter_3*.md")) + list(workspace_dir.glob("chapter_3*.md"))
                                if chapter3_files:
                                    with open(chapter3_files[0], 'r', encoding='utf-8') as f:
                                        methodology_content = f.read()
                                    print(f"ðŸ“‹ Read Chapter 3: {len(methodology_content)} chars")
                                else:
                                    # Use the just-generated chapter3_result
                                    methodology_content = chapter3_result
                                    print(f"ðŸ“‹ Using generated Chapter 3 content: {len(methodology_content)} chars")
                            except Exception as ch3_err:
                                print(f"âš ï¸ Could not read Chapter 3: {ch3_err}")
                                methodology_content = chapter3_result if chapter3_result else ""
                            
                            # Use actual workspace directory
                            workspace_path = Path(f"/tmp/workspaces/default")
                            workspace_path.mkdir(parents=True, exist_ok=True)
                            
                            # Create enhanced appendix generator with methodology
                            appendix_gen = AppendixGenerator(
                                workspace_dir=str(workspace_path),
                                topic=topic,
                                case_study=case_study,
                                objectives=objectives,
                                methodology_content=methodology_content,
                                research_questions=research_questions
                            )
                            
                            await events.publish(job_id, "log", {
                                "message": f"ðŸ“‹ Detected: {appendix_gen.research_design} design with {', '.join(appendix_gen.data_collection_methods)}"
                            }, session_id=session_id)
                            
                            # Generate all appropriate appendices
                            generated_files = await appendix_gen.generate_all_appendices()
                            
                            await events.publish(job_id, "log", {
                                "message": f"âœ… Generated {len(generated_files)} study tool(s): {', '.join([Path(f).stem for f in generated_files])}"
                            }, session_id=session_id)
                            
                        except Exception as appendix_error:
                            print(f"âš ï¸ Appendix generation failed: {appendix_error}")
                            import traceback
                            traceback.print_exc()
                            await events.publish(job_id, "log", {"message": f"âš ï¸ Appendix generation skipped: {appendix_error}"}, session_id=session_id)
                        
                        # Combine appendices into main proposal
                        if generated_files:
                            await events.publish(job_id, "log", {"message": "ðŸ“‹ Combining appendices into proposal..."}, session_id=session_id)
                            
                            appendix_section = "\n\n---\n\n# APPENDICES\n\n"
                            
                            for file_path in generated_files:
                                try:
                                    with open(file_path, 'r', encoding='utf-8') as f:
                                        appendix_content = f.read()
                                    
                                    # Add page break before each appendix
                                    appendix_section += "\n\n---\n\n"
                                    appendix_section += appendix_content
                                    
                                except Exception as read_error:
                                    print(f"Error reading appendix {file_path}: {read_error}")
                            
                            full_proposal_with_refs += appendix_section
                        
                        # Save individual chapter files to workspace
                        await events.publish(job_id, "log", {"message": "ðŸ’¾ Saving files to workspace..."}, session_id=session_id)
                        
                        try:
                            from pathlib import Path
                            workspace_path = Path(f"/tmp/workspaces/default")
                            workspace_path.mkdir(parents=True, exist_ok=True)
                            
                            # Save Chapter 1
                            chapter1_file = workspace_path / "Chapter_1_Introduction.md"
                            with open(chapter1_file, 'w', encoding='utf-8') as f:
                                f.write(f"# CHAPTER 1: INTRODUCTION\n\n{chapter1_result}")
                            
                            # Save Chapter 2
                            chapter2_file = workspace_path / "Chapter_2_Literature_Review.md"
                            with open(chapter2_file, 'w', encoding='utf-8') as f:
                                f.write(f"# CHAPTER 2: LITERATURE REVIEW\n\n{chapter2_result}")
                            
                            # Save Chapter 3
                            chapter3_file = workspace_path / "Chapter_3_Methodology.md"
                            with open(chapter3_file, 'w', encoding='utf-8') as f:
                                f.write(f"# CHAPTER 3: RESEARCH METHODOLOGY\n\n{chapter3_result}")
                            
                            # Save References
                            references_file = workspace_path / "References.md"
                            with open(references_file, 'w', encoding='utf-8') as f:
                                f.write(references_section)
                            
                            # Save complete proposal
                            proposal_file = workspace_path / "Complete_Proposal.md"
                            with open(proposal_file, 'w', encoding='utf-8') as f:
                                f.write(f"# RESEARCH PROPOSAL\n\n**Topic:** {topic}\n\n**Case Study:** {case_study}\n\n**Date:** {datetime.now().strftime('%B %Y')}\n\n---\n\n")
                                f.write(full_proposal_with_refs)
                            
                            await events.publish(job_id, "log", {"message": f"âœ… Saved 5 files to workspace/default/"}, session_id=session_id)
                            
                        except Exception as save_error:
                            print(f"âš ï¸ File saving failed: {save_error}")
                            import traceback
                            traceback.print_exc()
                        
                        await events.publish(
                            job_id,
                            "response_chunk",
                            {"chunk": f"\n\nâœ… **Full Research Proposal Complete!**\n\nAll 3 chapters generated successfully with:\n- âœ… Consistent objectives throughout\n- âœ… {len(sorted_citations)} unique references consolidated\n- âœ… Unified References section\n- âœ… {len(generated_files)} study tool appendices\n- âœ… All files saved to workspace\n\n**Files Created:**\n- Complete_Proposal.md (all-in-one)\n- Chapter_1_Introduction.md\n- Chapter_2_Literature_Review.md\n- Chapter_3_Methodology.md\n- References.md\n- appendices/ folder with {len(generated_files)} tool(s)\n", "accumulated": full_proposal_with_refs},
                            session_id=session_id
                        )
                        
                        await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "success"}, session_id=session_id)
                        
                    # Individual chapter generation
                    elif chapter_type == "chapter_three_generate":
                        # Generate Chapter Three - Research Methodology
                        await parallel_chapter_generator.generate_chapter_three(
                            topic=topic,
                            case_study=case_study,
                            job_id=job_id,
                            session_id=request.session_id,
                            objectives=objectives,
                            research_questions=research_questions
                        )
                    elif chapter_type == "chapter_two_generate":
                        # Generate Chapter Two - Literature Review
                        await parallel_chapter_generator.generate_chapter_two(
                            topic=topic,
                            case_study=case_study,
                            job_id=job_id,
                            session_id=request.session_id,
                            objectives=objectives,
                            research_questions=research_questions
                        )
                    else:
                        # Generate Chapter One
                        await parallel_chapter_generator.generate(
                            topic=topic,
                            case_study=case_study,
                            job_id=job_id,
                            session_id=request.session_id,
                            background_style=background_style
                        )
                except Exception as e:
                    print(f"Chapter generation error: {e}")
                    import traceback
                    traceback.print_exc()
                    await events.publish(job_id, "response_chunk", {"chunk": f"Error generating chapter: {str(e)}", "accumulated": f"Error: {str(e)}"}, session_id=request.session_id)
                    await events.publish(job_id, "stage_completed", {"stage": "complete", "status": "error"}, session_id=request.session_id)
            
            asyncio.create_task(run_chapter_generation())
            
            if chapter_type == "chapter_two_generate":
                return {"response": f"ðŸ“š Starting Chapter Two (Literature Review) generation for: {topic}...\\n\\n10+ research agents + 20+ writer agents working simultaneously!\\nObjective-based themes generated automatically!", "job_id": job_id, "plan": [], "reasoning": ""}
            else:
                return {"response": f"ðŸ“– Starting parallel chapter generation for: {topic}...\\n\\n6 research agents + 5 writer agents working simultaneously!", "job_id": job_id, "plan": [], "reasoning": ""}
        
        # ROUTE 7.5: DATASET GENERATION - AI synthetic data collection
        if route_info["route"] == "pipeline" and route_info.get("params", {}).get("generation_type") == "dataset_generate":
            from services.data_collection_worker import generate_research_dataset
            import asyncio
            from pathlib import Path
            import glob
            
            sample_size = route_info.get("params", {}).get("sample_size")
            
            async def run_dataset_generation():
                try:
                    await events.connect()
                    session_id = request.session_id
                    
                    await events.publish(job_id, "response_chunk", {
                        "chunk": f"# ðŸ“Š Generating Synthetic Research Dataset\n\n**Simulating survey data collection...**\n\n",
                        "accumulated": ""
                    }, session_id=session_id)
                    
                    # Get topic from session database
                    topic = ""
                    case_study = ""
                    objectives = []
                    
                    try:
                        from services.thesis_session_db import ThesisSessionDB
                        db = ThesisSessionDB(session_id)
                        topic = db.get_topic() or "Research Study"
                        case_study = db.get_case_study() or ""
                        obj_data = db.get_objectives() or {}
                        objectives = obj_data.get("specific", [])
                        if obj_data.get("general"):
                            objectives = [obj_data["general"]] + objectives
                    except Exception as e:
                        print(f"âš ï¸ Could not get session data: {e}")
                        topic = "Research Study"
                    
                    # Provide default objectives if none found
                    if not objectives:
                        objectives = [
                            f"To assess the current state of {topic[:40]}",
                            f"To identify challenges related to {topic[:40]}",
                            f"To examine stakeholder perceptions on {topic[:40]}",
                            f"To recommend improvements for {topic[:40]}"
                        ]
                        print(f"ðŸ“‹ Using default objectives for dataset generation")
                    
                    await events.publish(job_id, "log", {
                        "message": f"ðŸ“‹ Topic: {topic[:50]}... | {len(objectives)} objectives"
                    }, session_id=session_id)
                    
                    # Find questionnaire file
                    questionnaire_path = None
                    methodology_path = None
                    
                    workspace_dir = Path("/tmp/workspaces/default/appendices")
                    thesis_dir = Path("/home/gemtech/Desktop/thesis/thesis_data/default")
                    
                    # Look for questionnaire
                    for search_dir in [workspace_dir, thesis_dir]:
                        questionnaire_files = list(search_dir.glob("*Questionnaire*.md"))
                        if questionnaire_files:
                            questionnaire_path = str(questionnaire_files[0])
                            break
                    
                    # Look for methodology
                    chapter3_files = list(thesis_dir.glob("Chapter_3*.md"))
                    if chapter3_files:
                        methodology_path = str(chapter3_files[0])
                    
                    if questionnaire_path:
                        await events.publish(job_id, "log", {
                            "message": f"ðŸ“‹ Found questionnaire: {Path(questionnaire_path).name}"
                        }, session_id=session_id)
                    
                    # Output directory - save with other thesis files
                    output_dir = "/home/gemtech/Desktop/thesis/workspaces/default/datasets"
                    os.makedirs(output_dir, exist_ok=True)
                    
                    # Generate dataset
                    await events.publish(job_id, "log", {
                        "message": f"ðŸš€ Deploying AI respondent agents..."
                    }, session_id=session_id)
                    
                    result = await generate_research_dataset(
                        topic=topic,
                        case_study=case_study,
                        questionnaire_path=questionnaire_path,
                        methodology_path=methodology_path,
                        sample_size=sample_size,
                        objectives=objectives,
                        job_id=job_id,
                        session_id=session_id,
                        output_dir=output_dir
                    )
                    
                    csv_path = result["csv_path"]
                    spss_path = result["spss_syntax_path"]
                    stats = result["stats"]
                    xlsx_path = stats.get("xlsx_path")
                    
                    # Add XLSX to files list if exists
                    if xlsx_path:
                        result['files'].append(xlsx_path)
                    
                    # Summary message
                    summary = f"""
## âœ… Dataset Generated Successfully!

### Files Created:
- ðŸ“„ **CSV Dataset**: `{Path(csv_path).name}`
- ðŸ“Š **Excel Dataset**: `{Path(xlsx_path).name if xlsx_path else 'N/A'}`
- ðŸ“‹ **SPSS Syntax**: `{Path(spss_path).name}`

### Statistics:
- **Sample Size**: {result['sample_size']} respondents
- **Total Variables**: {result['total_variables']}

### Demographic Distribution:
"""
                    for var, freq in list(stats.get("demographics", {}).items())[:3]:
                        summary += f"\n**{var}**:\n"
                        for val, count in list(freq.items())[:5]:
                            pct = (count / result['sample_size']) * 100
                            summary += f"  - {val}: {count} ({pct:.1f}%)\n"
                    
                    # Add all generated files
                    summary += "\n### All Generated Files:\n"
                    for file_path in result.get('files', []):
                        summary += f"- ðŸ“„ `{Path(file_path).name}`\n"
                    
                    summary += f"""
### Next Steps:
1. Download the CSV files
2. Import questionnaire data into SPSS using the provided syntax
3. Analyze interview/FGD transcripts using thematic analysis
4. Review observation data for patterns

**Note**: This is synthetic data for demonstration purposes.
"""
                    
                    await events.publish(job_id, "response_chunk", {
                        "chunk": summary,
                        "accumulated": summary
                    }, session_id=session_id)
                    
                    # Publish all files
                    for file_path in result.get('files', []):
                        await events.publish(job_id, "file_created", {
                            "path": file_path,
                            "name": Path(file_path).name
                        }, session_id=session_id)
                    
                    await events.publish(job_id, "stage_completed", {
                        "stage": "complete",
                        "status": "success"
                    }, session_id=session_id)
                    
                except Exception as e:
                    import traceback
                    traceback.print_exc()
                    await events.publish(job_id, "response_chunk", {
                        "chunk": f"âŒ Error generating dataset: {str(e)}",
                        "accumulated": f"Error: {str(e)}"
                    }, session_id=request.session_id)
                    await events.publish(job_id, "stage_completed", {
                        "stage": "complete",
                        "status": "error"
                    }, session_id=request.session_id)
            
            asyncio.create_task(run_dataset_generation())
            sample_info = f" with {sample_size} respondents" if sample_size else ""
            return {
                "response": f"ðŸ“Š Starting synthetic dataset generation{sample_info}...\\n\\nAI agents simulating survey responses!",
                "job_id": job_id,
                "plan": [],
                "reasoning": ""
            }
        
        # ROUTE 7.6: CHAPTER 4 GENERATION - Data Presentation and Analysis
        if route_info["route"] == "pipeline" and route_info.get("params", {}).get("generation_type") == "chapter_four_generate":
            from services.chapter4_generator import generate_chapter4
            import asyncio
            from pathlib import Path
            
            async def run_chapter4_generation():
                try:
                    await events.connect()
                    session_id = request.session_id
                    
                    await events.publish(job_id, "response_chunk", {
                        "chunk": f"# ðŸ“Š Generating Chapter 4: Data Presentation and Analysis\n\n**Building tables, figures, and interpretation...**\n\n",
                        "accumulated": ""
                    }, session_id=session_id)
                    
                    # Get session data
                    topic = ""
                    case_study = ""
                    objectives = []
                    
                    try:
                        from services.thesis_session_db import ThesisSessionDB
                        db = ThesisSessionDB(session_id)
                        topic = db.get_topic() or "Research Study"
                        case_study = db.get_case_study() or ""
                        obj_data = db.get_objectives() or {}
                        objectives = obj_data.get("specific", [])
                        if obj_data.get("general"):
                            objectives = [obj_data["general"]] + objectives
                    except Exception as e:
                        print(f"âš ï¸ Could not get session data: {e}")
                        topic = "Research Study"
                    
                    # Generate default objectives if none found
                    if not objectives:
                        objectives = [
                            f"To assess the current state of {topic[:40]}",
                            f"To identify challenges related to {topic[:40]}",
                            f"To examine stakeholder perceptions on {topic[:40]}",
                            f"To recommend improvements for {topic[:40]}"
                        ]
                    
                    await events.publish(job_id, "log", {
                        "message": f"ðŸ“‹ Topic: {topic[:50]}... | {len(objectives)} objectives"
                    }, session_id=session_id)
                    
                    await events.publish(job_id, "log", {
                        "message": f"ðŸ“Š Loading datasets and generating analysis..."
                    }, session_id=session_id)
                    
                    # Generate Chapter 4
                    result = await generate_chapter4(
                        topic=topic,
                        case_study=case_study,
                        objectives=objectives,
                        job_id=job_id,
                        session_id=session_id
                    )
                    
                    filepath = result["filepath"]
                    tables = result["tables"]
                    figures = result["figures"]
                    
                    # Summary message
                    summary = f"""
## âœ… Chapter 4 Generated Successfully!

### File Created:
- ðŸ“„ **Chapter 4**: `{Path(filepath).name}`

### Content Summary:
- **Tables Generated**: {tables}
- **Figures Generated**: {figures}
- **Objectives Covered**: {result['objectives_covered']}

### Sections Included:
- 4.0 Introduction
- 4.1 Study Tools Rate of Return  
- 4.2 Demographics (all subsections)
- 4.3-4.{len(objectives)+2} Objective Analyses
  - Descriptive Statistics
  - Inferential Statistics
  - Qualitative Findings (quotes)
  - Triangulation
- Summary of Findings

**Note**: Review and customize the auto-generated content as needed.
"""
                    
                    await events.publish(job_id, "response_chunk", {
                        "chunk": summary,
                        "accumulated": summary
                    }, session_id=session_id)
                    
                    await events.publish(job_id, "file_created", {
                        "path": filepath,
                        "name": Path(filepath).name
                    }, session_id=session_id)
                    
                    await events.publish(job_id, "stage_completed", {
                        "stage": "complete",
                        "status": "success"
                    }, session_id=session_id)
                    
                except Exception as e:
                    import traceback
                    traceback.print_exc()
                    await events.publish(job_id, "response_chunk", {
                        "chunk": f"âŒ Error generating Chapter 4: {str(e)}",
                        "accumulated": f"Error: {str(e)}"
                    }, session_id=request.session_id)
                    await events.publish(job_id, "stage_completed", {
                        "stage": "complete",
                        "status": "error"
                    }, session_id=request.session_id)
            
            asyncio.create_task(run_chapter4_generation())
            return {
                "response": f"ðŸ“Š Starting Chapter 4 generation...\\n\\nCreating data presentation with tables, figures, and interpretation!",
                "job_id": job_id,
                "plan": [],
                "reasoning": ""
            }
        
        # ROUTE 8: Instant responses (greetings)
        if route_info["route"] == "instant":
            greetings = [
                "Hello! I'm your AI research assistant. How can I help you today?\n\nI can:\n- ðŸ” Search for images, papers, and web content\n- ðŸ“ Write essays, documents, and reports\n- ðŸ“š Research and synthesize literature\n- ðŸ’¬ Answer questions directly\n\nJust tell me what you need!",
                "Hi there! Ready to help with your research. What would you like to do?",
                "Hey! I'm here to assist. What can I help you with today?"
            ]
            import random
            response_text = random.choice(greetings)
            
            return {
                "response": response_text,
                "reasoning": "",
                "plan": [],
                "job_id": job_id
            }
        
        # Simple greetings - direct reply with streaming, no planning
        if is_simple_greeting(request.message):
            try:
                await events.connect()
                await events.log(job_id, "ðŸ’¬ Responding to simple query...", "info", session_id=request.session_id)
                
                # Stream a simple response
                greetings = [
                    "Hello! I'm your AI research assistant. I can help you with:\n- Writing essays and academic content\n- Research and finding papers\n- Creating and managing files\n- Generating images\n- And much more!\n\nJust tell me what you'd like to do!",
                    "Hi there! I'm here to help with your research. What would you like to work on?",
                    "Hey! Ready to assist with your research. What can I do for you?"
                ]
                import random
                response_text = random.choice(greetings)
                
                # Stream response word by word for real-time effect
                words = response_text.split(' ')
                accumulated = ""
                for i, word in enumerate(words):
                    accumulated += word + (" " if i < len(words) - 1 else "")
                    await events.publish(job_id, "response_chunk", {
                        "chunk": word + " ",
                        "accumulated": accumulated
                    }, session_id=request.session_id)
                    await asyncio.sleep(0.03)  # Small delay for streaming effect
                
                await events.publish(job_id, "stage_completed", {
                    "stage": "response",
                    "metadata": {"type": "simple"}
                }, session_id=request.session_id)
            except Exception as e:
                print(f"âš ï¸ Error in simple response streaming: {e}", flush=True)
            
            return {
                "response": "Hello! I'm your AI research assistant. I can help you with:\n- Writing essays and academic content\n- Research and finding papers\n- Creating and managing files\n- Generating images\n- And much more!\n\nJust tell me what you'd like to do!",
                "reasoning": "",
                "plan": [],
                "tool_results": {},
                "job_id": job_id
            }
        
        # Direct image generation - bypass planner (only for explicit generation requests)
        if is_image_generation_request(request.message):
            try:
                from services.image_generation import image_generation_service
                
                # Extract prompt (remove generation keywords)
                prompt = request.message
                for keyword in ["generate image", "create image", "make image", "generate a picture", "create a picture"]:
                    prompt = prompt.replace(keyword, "").replace(keyword.replace(" ", ""), "").strip()
                if not prompt:
                    prompt = request.message  # Use full message if no prompt extracted
                
                # Generate image directly
                result = await image_generation_service.generate(
                    prompt=prompt,
                    size="1024x1024"
                )
                
                if result.get("success"):
                    image_url = result.get("image_url") or result.get("url")
                    image_data = result.get("image_data")  # Base64 if available
                    
                    response_text = f"âœ… Image generated successfully!\n\nPrompt: {prompt}"
                    
                    return {
                        "response": "Image generated successfully!",
                        "reasoning": "",  # Empty - don't show reasoning panel
                        "plan": [],
                        "image_generation": {
                            "success": True,
                            "image_url": image_url,
                            "image_data": image_data,
                            "prompt": prompt
                        }
                    }
                else:
                    error_msg = result.get("error", "Image generation failed")
                    return {
                        "response": f"âŒ Failed to generate image: {error_msg}",
                        "reasoning": "",  # Empty - don't show reasoning panel
                        "plan": [],
                        "image_generation": {"success": False, "error": error_msg}
                    }
            except Exception as e:
                import traceback
                traceback.print_exc()
                return {
                    "response": f"âŒ Error generating image: {str(e)}",
                    "reasoning": f"Image generation error: {str(e)}",
                    "plan": []
                }
        
        # =====================================================================
        # INTELLIGENT RESEARCH SYNTHESIS - Search + Analyze + Write
        # =====================================================================
        is_synthesis, synthesis_topic, synthesis_style = is_research_synthesis_request(request.message)
        if is_synthesis and synthesis_topic:
            print(f"ðŸ§  Detected research synthesis request: '{synthesis_topic}' (style: {synthesis_style})")
            
            async def run_intelligent_synthesis():
                """
                Intelligent research synthesis pipeline with streaming.
                
                This pipeline THINKS about what to do, not just follows steps:
                1. Understands the research topic and scope
                2. Searches for relevant papers dynamically
                3. Analyzes gaps and themes in the literature
                4. Generates a coherent synthesis with proper citations
                """
                from services.sources_service import sources_service
                from services.literature_synthesis import literature_synthesis_service
                from services.planner import planner_service
                
                try:
                    # Connect to event system
                    await events.connect()
                    
                    # â”€â”€ PHASE 1: Understanding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    yield {"event": "stage", "data": json.dumps({
                        "stage": "understanding", 
                        "icon": "ðŸ§ ",
                        "message": f"Understanding research scope: {synthesis_topic}"
                    })}
                    
                    await events.log(job_id, f"ðŸ§  Analyzing research scope: {synthesis_topic}", "info", session_id=request.session_id)
                    await asyncio.sleep(0.3)
                    
                    # â”€â”€ PHASE 2: Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    yield {"event": "stage", "data": json.dumps({
                        "stage": "searching",
                        "icon": "ðŸ”", 
                        "message": f"Searching academic databases for papers on '{synthesis_topic}'..."
                    })}
                    
                    await events.log(job_id, f"ðŸ” Searching papers on: {synthesis_topic}", "info", session_id=request.session_id)
                    
                    search_result = await sources_service.search_and_save(
                        workspace_id=request.workspace_id,
                        query=synthesis_topic,
                        max_results=10,
                        auto_save=True
                    )
                    
                    papers_found = search_result.get('total_results', 0)
                    papers_saved = search_result.get('saved_count', 0)
                    
                    yield {"event": "search_complete", "data": json.dumps({
                        "papers_found": papers_found,
                        "papers_saved": papers_saved,
                        "papers": search_result.get('results', [])[:5]  # Preview first 5
                    })}
                    
                    await events.log(job_id, f"ðŸ“š Found {papers_found} papers, saved {papers_saved}", "info", session_id=request.session_id)
                    
                    if papers_saved == 0:
                        yield {"event": "content", "data": json.dumps({
                            "text": f"\n\nâš ï¸ No papers found for '{synthesis_topic}'. Try a broader or different search term.\n"
                        })}
                        yield {"event": "done", "data": json.dumps({"status": "no_sources"})}
                        return
                    
                    # â”€â”€ PHASE 3: Analysis & Planning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    yield {"event": "stage", "data": json.dumps({
                        "stage": "planning",
                        "icon": "ðŸ“‹",
                        "message": "Analyzing literature themes and planning synthesis structure..."
                    })}
                    
                    await events.log(job_id, "ðŸ“‹ Planning synthesis structure...", "info", session_id=request.session_id)
                    
                    # Quick outline using planner
                    try:
                        outline = await planner_service.generate_outline(
                            topic=synthesis_topic,
                            word_count=1500,
                            include_images=False,
                            job_id=job_id
                        )
                        
                        if outline and outline.get('sections'):
                            yield {"event": "outline", "data": json.dumps({
                                "title": outline.get('title', synthesis_topic),
                                "sections": [s.get('title', s) if isinstance(s, dict) else s for s in outline.get('sections', [])]
                            })}
                    except Exception as e:
                        print(f"âš ï¸ Outline generation skipped: {e}")
                    
                    # â”€â”€ PHASE 4: Writing Synthesis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    yield {"event": "stage", "data": json.dumps({
                        "stage": "writing",
                        "icon": "âœï¸",
                        "message": "Generating comprehensive literature synthesis with citations..."
                    })}
                    
                    await events.log(job_id, "âœï¸ Writing synthesis with citations...", "info", session_id=request.session_id)
                    
                    # Stream the synthesis content
                    full_content = ""
                    async for chunk in literature_synthesis_service.synthesize_literature(
                        workspace_id=request.workspace_id,
                        topic=synthesis_topic,
                        output_format="markdown"
                    ):
                        full_content += chunk
                        yield {"event": "content", "data": json.dumps({"text": chunk})}
                    
                    # â”€â”€ PHASE 5: Complete â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    yield {"event": "stage", "data": json.dumps({
                        "stage": "complete",
                        "icon": "âœ…",
                        "message": "Synthesis complete!"
                    })}
                    
                    await events.log(job_id, "âœ… Research synthesis complete!", "info", session_id=request.session_id)
                    
                    # Save to workspace
                    try:
                        from services.workspace_service import WORKSPACES_DIR
                        output_path = WORKSPACES_DIR / request.workspace_id / f"synthesis_{synthesis_topic.replace(' ', '_')[:30]}.md"
                        output_path.parent.mkdir(parents=True, exist_ok=True)
                        output_path.write_text(full_content, encoding='utf-8')
                        
                        yield {"event": "file_created", "data": json.dumps({
                            "path": str(output_path),
                            "name": output_path.name
                        })}
                        await events.log(job_id, f"ðŸ“„ Saved to: {output_path.name}", "info", session_id=request.session_id)
                    except Exception as e:
                        print(f"âš ï¸ Could not save synthesis: {e}")
                    
                    yield {"event": "done", "data": json.dumps({
                        "status": "success",
                        "papers_used": papers_saved,
                        "word_count": len(full_content.split())
                    })}
                    
                except Exception as e:
                    import traceback
                    traceback.print_exc()
                    yield {"event": "error", "data": json.dumps({"message": str(e)})}
            
            return EventSourceResponse(run_intelligent_synthesis())
        
        # Paper/Literature search - route directly to sources service
        if is_paper_search_request(request.message):
            try:
                await events.connect()
                await events.log(job_id, "ðŸ“š Searching academic papers...", "info", session_id=request.session_id)
                
                from services.sources_service import sources_service
                import re
                
                # Extract search query from message
                query = request.message
                for phrase in ["search for papers on", "search papers on", "find papers on",
                               "search for papers about", "search papers about", "find papers about",
                               "search for articles on", "find articles on", "search literature on",
                               "find research on", "look for papers on", "search academic",
                               "search for sources on", "find sources on"]:
                    query = re.sub(phrase, "", query, flags=re.IGNORECASE)
                query = query.strip()
                
                if not query or len(query) < 3:
                    query = request.message  # Use full message if extraction failed
                
                # Search and auto-save
                result = await sources_service.search_and_save(
                    workspace_id=request.workspace_id,
                    query=query,
                    max_results=5,
                    auto_save=True
                )
                
                # Format response
                response_parts = [f"ðŸ“š **Found {result['total_results']} papers for:** {query}\n"]
                
                for i, paper in enumerate(result['results'][:5], 1):
                    authors = paper.get('authors', [])
                    if authors:
                        author_str = authors[0] if isinstance(authors[0], str) else authors[0].get('name', 'Unknown')
                        if len(authors) > 1:
                            author_str += " et al."
                    else:
                        author_str = "Unknown"
                    
                    response_parts.append(f"**{i}. {paper.get('title', 'Untitled')}**")
                    response_parts.append(f"   - *{author_str}* ({paper.get('year', 'N/A')})")
                    if paper.get('venue'):
                        response_parts.append(f"   - Venue: {paper.get('venue')}")
                    if paper.get('citation_count', 0) > 0:
                        response_parts.append(f"   - Citations: {paper.get('citation_count')}")
                    response_parts.append("")
                
                if result['saved_count'] > 0:
                    response_parts.append(f"âœ… **Saved {result['saved_count']} papers** to `sources/` folder")
                    response_parts.append("Access via GraphQL at `/graphql` or REST at `/api/workspace/{id}/sources`")
                
                response_text = "\n".join(response_parts)
                
                # Stream response
                await events.publish(job_id, "response_chunk", {
                    "chunk": response_text,
                    "accumulated": response_text
                }, session_id=request.session_id)
                
                await events.publish(job_id, "stage_completed", {
                    "stage": "search",
                    "metadata": {"type": "paper_search", "saved": result['saved_count']}
                }, session_id=request.session_id)
                
                return {
                    "response": response_text,
                    "reasoning": "",
                    "plan": [],
                    "search_results": result,
                    "job_id": job_id
                }
            except Exception as e:
                import traceback
                traceback.print_exc()
                return {
                    "response": f"âŒ Paper search error: {str(e)}",
                    "reasoning": "",
                    "plan": [],
                    "job_id": job_id
                }
        
        # PDF action (summarize, read, analyze) - direct PDF processing
        is_pdf_action, pdf_action_type, pdf_name = is_pdf_action_request(request.message)
        if is_pdf_action:
            try:
                await events.connect()
                await events.log(job_id, f"ðŸ“„ Processing PDF ({pdf_action_type})...", "info", session_id=request.session_id)
                
                from services.pdf_service import get_pdf_service
                from services.deepseek_direct import deepseek_direct
                import glob
                
                pdf_service = get_pdf_service()
                workspace_path = WORKSPACES_DIR / request.workspace_id
                
                # Find the PDF file
                pdf_path = None
                if pdf_name:
                    # Search for the specific PDF
                    for pfile in workspace_path.rglob("*.pdf"):
                        if pdf_name.lower() in pfile.name.lower():
                            pdf_path = pfile
                            break
                
                if not pdf_path:
                    # Try to find any PDF in workspace (most recent)
                    pdf_files = list(workspace_path.rglob("*.pdf"))
                    if pdf_files:
                        # Sort by modification time, newest first
                        pdf_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                        pdf_path = pdf_files[0]
                
                if not pdf_path or not pdf_path.exists():
                    return {
                        "response": "âŒ No PDF file found in workspace. Please upload a PDF first.",
                        "reasoning": "",
                        "plan": [],
                        "job_id": job_id
                    }
                
                await events.log(job_id, f"ðŸ“„ Reading: {pdf_path.name}", "info", session_id=request.session_id)
                
                # Extract text from PDF
                text = pdf_service.extract_text_simple(pdf_path)
                
                if not text or len(text) < 50:
                    return {
                        "response": f"âŒ Could not extract text from {pdf_path.name}. The PDF may be image-based or encrypted.",
                        "reasoning": "",
                        "plan": [],
                        "job_id": job_id
                    }
                
                # Truncate if too long
                max_chars = 15000
                if len(text) > max_chars:
                    text = text[:max_chars] + f"\n\n[... Truncated. Full document is {len(text)} characters ...]"
                
                # Create prompt based on action type
                if pdf_action_type == "summarize":
                    prompt = f"""Please provide a comprehensive summary of the following document. Include:
1. Main topic and purpose
2. Key findings or arguments
3. Important conclusions

Document content:
---
{text}
---

Please provide a clear, well-structured summary."""

                elif pdf_action_type == "extract":
                    prompt = f"""Extract the key points from the following document. Format as a bulleted list of the most important information:

Document content:
---
{text}
---

Key Points:"""

                elif pdf_action_type == "analyze":
                    prompt = f"""Provide an analysis of the following document, including:
1. Main themes and topics
2. Methodology (if applicable)
3. Strengths and limitations
4. Key insights

Document content:
---
{text}
---

Analysis:"""

                else:  # read
                    prompt = f"""Based on the following document, provide a helpful overview:

Document content:
---
{text}
---

Overview:"""
                
                # Generate response using LLM
                response_text = f"ðŸ“„ **{pdf_action_type.capitalize()}:** {pdf_path.name}\n\n"
                
                async for chunk in deepseek_direct.generate_stream(
                    prompt=prompt,
                    system_prompt="You are an expert document analyst. Provide clear, well-structured responses. Use markdown formatting."
                ):
                    response_text += chunk
                    await events.publish(job_id, "response_chunk", {
                        "chunk": chunk,
                        "accumulated": response_text
                    }, session_id=request.session_id)
                
                await events.publish(job_id, "stage_completed", {
                    "stage": "response",
                    "metadata": {"type": "pdf_action", "action": pdf_action_type, "file": pdf_path.name}
                }, session_id=request.session_id)
                
                return {
                    "response": response_text,
                    "reasoning": "",
                    "plan": [],
                    "job_id": job_id
                }
            except Exception as e:
                import traceback
                traceback.print_exc()
                return {
                    "response": f"âŒ PDF processing error: {str(e)}",
                    "reasoning": "",
                    "plan": [],
                    "job_id": job_id
                }
        
        # Simple questions - direct AI response, no planning
        if is_simple_question(request.message):
            try:
                await events.connect()
                await events.log(job_id, "ðŸ’¬ Responding to simple question...", "info", session_id=request.session_id)
                
                # Use DeepSeek for direct response
                from services.deepseek_direct import deepseek_direct
                
                response_text = ""
                async for chunk in deepseek_direct.generate_stream(
                    prompt=request.message,
                    system_prompt="You are a helpful AI assistant. Provide clear, concise, and accurate responses. For math equations, use LaTeX notation with $ for inline and $$ for block equations."
                ):
                    response_text += chunk
                    # Stream response chunks
                    await events.publish(job_id, "response_chunk", {
                        "chunk": chunk,
                        "accumulated": response_text
                    }, session_id=request.session_id)
                
                await events.publish(job_id, "stage_completed", {
                    "stage": "response",
                    "metadata": {"type": "simple_question"}
                }, session_id=request.session_id)
                
                return {
                    "response": response_text,
                    "reasoning": "",  # Empty - no planning visible
                    "plan": [],
                    "tool_results": {},
                    "job_id": job_id
                }
            except Exception as e:
                import traceback
                traceback.print_exc()
                print(f"âš ï¸ Simple question direct response failed: {e}, falling through to planner")
                # Fall through to task classifier if direct response fails
        
        # Use intelligent task classifier
        from services.task_classifier import task_classifier
        task_info = task_classifier.classify(request.message)
        
        # For complex tasks, use the INTELLIGENT ORCHESTRATOR (live streaming)
        if task_info["strategy"] == "worker":
            print(f"ðŸ§  Complex task detected, using Intelligent Orchestrator")
            
            async def run_orchestrator():
                """Stream intelligent agent actions to frontend."""
                from services.intelligent_orchestrator import intelligent_orchestrator
                
                try:
                    async for event in intelligent_orchestrator.run(
                        task=request.message,
                        workspace_id=request.workspace_id,
                        session_id=request.session_id,
                        job_id=job_id
                    ):
                        yield event
                except Exception as e:
                    import traceback
                    traceback.print_exc()
                    yield {"event": "error", "data": json.dumps({"message": str(e)})}
            
            return EventSourceResponse(run_orchestrator())
        
        # Handle mentioned agents - queue jobs to appropriate workers
        if request.mentioned_agents and len(request.mentioned_agents) > 0:
            try:
                await events.connect()
                
                # Map agent names to worker queues
                agent_to_queue = {
                    "objective": "objectives",
                    "objectives": "objectives",
                    "planner": "objectives",  # Planner can use objectives worker
                    "writer": "content",
                    "content": "content",
                    "editor": "content",  # Editor uses content worker
                    "research": "search",
                    "search": "search",
                    "citation": "search",  # Citation uses search worker
                }
                
                # Queue jobs to workers based on mentioned agents
                from core.queue import JobQueue
                queued_jobs = []
                
                for agent_name in request.mentioned_agents:
                    agent_lower = agent_name.lower().replace("_", "").replace("-", "")
                    queue_name = agent_to_queue.get(agent_lower)
                    
                    if queue_name:
                        try:
                            # Prepare job data based on agent type
                            job_data = {
                                "job_id": job_id,
                                "message": request.message,
                                "workspace_id": request.workspace_id,
                                "session_id": request.session_id,
                                "user_id": request.user_id,
                                "mentioned_agent": agent_name,
                            }
                            
                            # Add agent-specific data
                            if queue_name == "objectives":
                                job_data.update({
                                    "thesis_id": request.workspace_id,
                                    "topic": request.message,
                                    "mode": "voting"
                                })
                            elif queue_name == "content":
                                job_data.update({
                                    "thesis_id": request.workspace_id,
                                    "type": "chapter",
                                    "section_title": "Content"
                                })
                            elif queue_name == "search":
                                job_data.update({
                                    "thesis_id": request.workspace_id,
                                    "query": request.message,
                                    "type": "papers",
                                    "max_results": 20
                                })
                            
                            # Queue the job
                            queued_job_id = await JobQueue.push(
                                queue_name=queue_name,
                                data=job_data,
                                job_id=f"{job_id}-{agent_name}",
                                priority="high"  # High priority for mentioned agents
                            )
                            
                            queued_jobs.append({"agent": agent_name, "queue": queue_name, "job_id": queued_job_id})
                            print(f"ðŸ“¤ Queued job for @{agent_name} to {queue_name} worker", flush=True)
                            
                        except Exception as queue_error:
                            print(f"âš ï¸ Failed to queue job for {agent_name}: {queue_error}", flush=True)
                            import traceback
                            traceback.print_exc()
                
                if queued_jobs:
                    await events.publish(job_id, "agents_mentioned", {
                        "agents": request.mentioned_agents,
                        "queued_jobs": queued_jobs,
                        "message": f"Queued {len(queued_jobs)} jobs to workers"
                    })
                    print(f"âœ… Queued {len(queued_jobs)} jobs to workers for mentioned agents", flush=True)
                
            except Exception as e:
                print(f"âš ï¸ Error handling mentioned agents: {e}", flush=True)
                import traceback
                traceback.print_exc()
                pass
        
        # Emit initial event immediately - ensure connection first
        try:
            await events.connect()  # Ensure Redis connection is established
            # Don't log generic "starting" messages - let real content stream through
            # await events.log(job_id, "ðŸš€ Starting request processing...", "info")
            # await events.publish(job_id, "stage_started", {"stage": "planning", "message": "Generating plan..."})
        except Exception as event_error:
            print(f"âš ï¸ Failed to emit initial event: {event_error}", flush=True)
            import traceback
            traceback.print_exc()
            pass  # Don't fail if events fail
        
        # Generate plan for other requests
        try:
            # Check RAG for similar solutions
            from services.rag_system import rag_system
            similar_solutions = await rag_system.retrieve_similar(
                query=request.message,
                category="solutions",
                top_k=2
            )
            
            # Emit planning event
            try:
                await events.log(job_id, "ðŸ’­ Generating plan...", "info")
            except:
                pass
            
            # Add timeout to prevent hanging
            import asyncio
            
            # Create streaming callback for reasoning
            reasoning_chunks = []
            async def stream_reasoning_chunk(chunk: str):
                """Stream reasoning chunks to frontend"""
                reasoning_chunks.append(chunk)
                try:
                    await events.publish(job_id, "reasoning_chunk", {
                        "chunk": chunk,
                        "accumulated": "".join(reasoning_chunks)
                    })
                except:
                    pass
            
            # Emit immediate planning status
            try:
                await events.log(job_id, "ðŸ” Analyzing your request and creating a plan...", "info")
                await events.publish(job_id, "stage_started", {
                    "stage": "planning",
                    "message": "Analyzing request..."
                })
            except:
                pass
            
            # Generate plan with streaming support
            plan_data = await asyncio.wait_for(
            planner_service.generate_plan(
                user_request=request.message,
                session_id=request.session_id,
                workspace_id=request.workspace_id,
                user_id=request.user_id,
                job_id=job_id,
                stream=True,
                stream_callback=stream_reasoning_chunk,
                mentioned_agents=request.mentioned_agents or []
            ),
                timeout=60.0  # 60 second timeout for planning
            )
            
            # Don't log generic "plan generated" - let real content stream
            # Plan completion is handled by the actual content streaming
            # Ensure plan_data is a dict with required keys
            if not isinstance(plan_data, dict):
                plan_data = {"reasoning": "", "plan": []}
            if "reasoning" not in plan_data:
                plan_data["reasoning"] = ""
            if "plan" not in plan_data:
                plan_data["plan"] = []
        except asyncio.TimeoutError:
            print(f"âš ï¸ Planner timeout after 60 seconds")
            plan_data = {
                "reasoning": "Planning took too long. The request may be too complex. Please try breaking it into smaller steps.",
                "plan": []
            }
        except Exception as plan_error:
            import traceback
            traceback.print_exc()
            print(f"Planner error: {plan_error}")
            # Return a safe response instead of crashing
            plan_data = {
                "reasoning": f"Error generating plan: {str(plan_error)[:200]}",
                "plan": []
            }
        
        reasoning = plan_data.get("reasoning", "")
        plan = plan_data.get("plan", [])
        
        # Execute plan steps
        response_parts = []
        recent_actions = []
        
        tool_results = {}  # Store tool results by tool name for response
        essay_content = None
        essay_file_path = None
        
        # First pass: Execute all tools except save_file
        for step_idx, step in enumerate(plan):
            tool_name = step.get("tool")
            if not tool_name or tool_name == "save_file" or tool_name == "write_file":
                continue
            
            # Emit tool execution event
            arguments = step.get("arguments", {})
            try:
                # Show BEFORE starting, not after
                if tool_name == "web_search":
                    query = arguments.get("query", "")
                    await events.log(job_id, f"ðŸŒ Searching the web for: {query[:60]}...", "info")
                    
                    # Open Internet Search Agent tab
                    from core.agent_stream_factory import get_agent_stream_handler
                    search_handler = get_agent_stream_handler("internet_search", job_id, request.workspace_id)
                    await events.connect()
                    await events.publish(job_id, "agent_stream", {
                        "agent": "internet_search",
                        "tab_id": search_handler.tab_id,
                        "chunk": "",
                        "content": f"ðŸŒ **Internet Search Agent**\n\n_Searching for: {query}_\n\n",
                        "type": "search",
                        "completed": False,
                        "workspace_id": request.workspace_id,
                        "metadata": {
                            "status": "running",
                            "action": "searching",
                            "query": query
                        }
                    })
                else:
                    await events.log(job_id, f"ðŸ”§ Executing {tool_name} (step {step_idx + 1}/{len(plan)})...", "info")
                    
                await events.publish(job_id, "tool_started", {
                    "tool": tool_name,
                    "step": step_idx + 1,
                    "total": len(plan),
                    "description": step.get("description", "")
                })
            except:
                pass
            
            try:
                result = await execute_tool(tool_name, arguments, request.workspace_id)
                
                # Emit tool completion event
                try:
                    status = result.get("status", "unknown")
                    # Don't log generic "tool completed" - let real content stream
                    # await events.log(job_id, f"âœ“ {tool_name} completed ({status})", "info" if status == "success" else "warning")
                    await events.publish(job_id, "tool_completed", {
                        "tool": tool_name,
                        "step": step_idx + 1,
                        "status": status
                    })
                except:
                    pass
                
                if result.get("status") == "success":
                    recent_actions.append(f"{tool_name}: {step.get('description', '')}")
                    response_parts.append(f"âœ“ {step.get('description', tool_name)} completed.")
                    
                    # Store tool result by tool name for inclusion in response
                    if tool_name == "web_search" and result.get("results"):
                        tool_results[tool_name] = result.get("results", [])
                    elif tool_name == "image_search" and result.get("images"):
                        tool_results[tool_name] = result.get("images", [])
                    elif tool_name == "image_generate" and result.get("image_url"):
                        if "image_generate" not in tool_results:
                            tool_results["image_generate"] = []
                        tool_results["image_generate"].append({
                            "url": result.get("image_url"),
                            "prompt": arguments.get("prompt", "")
                        })
            except Exception as tool_error:
                print(f"Tool execution error for {tool_name}: {tool_error}")
                response_parts.append(f"âš  {step.get('description', tool_name)} failed: {str(tool_error)}")
        
        # Generate essay content ONLY if user ACTUALLY asked for an essay
        # Don't generate essays for image-only or search-only requests!
        import re
        user_wants_essay = bool(re.search(
            r"(write|create|generate|make)\s+(an?\s+)?(essay|document|report|paper|article|content)",
            request.message.lower()
        ))
        
        # Skip essay if user just wanted an image or search results
        user_wants_image_only = bool(re.search(
            r"(generat|creat|mak|draw)(e|ing)\s+.*?(image|picture|photo)",
            request.message.lower()
        )) and not user_wants_essay
        
        user_wants_search_only = (
            ("search" in request.message.lower() or "find" in request.message.lower()) and 
            "results" in request.message.lower() and
            not user_wants_essay
        )
        
        if tool_results and user_wants_essay and any(key in tool_results for key in ["web_search", "image_search", "image_generate"]):
            try:
                from services.deepseek_direct import deepseek_direct_service
                
                # Build context from tool results
                context_parts = []
                
                if "web_search" in tool_results and tool_results["web_search"]:
                    context_parts.append("## Research Findings:\n")
                    for result in tool_results["web_search"][:3]:  # Use top 3
                        if isinstance(result, dict):
                            title = result.get('title', '')
                            content = result.get('content', '') or result.get('snippet', '') or ''
                            if title or content:
                                context_parts.append(f"- {title}: {content[:200]}")
                
                # Build image markdown
                image_markdown = []
                if "image_search" in tool_results and tool_results["image_search"]:
                    for img in tool_results["image_search"][:1]:  # First searched image
                        if isinstance(img, dict):
                            img_url = img.get("url") or img.get("full") or img.get("image_url")
                            if img_url:
                                image_markdown.append(f"![Searched Image: {img.get('title', 'Uganda')}]({img_url})")
                
                if "image_generate" in tool_results and tool_results["image_generate"]:
                    for img in tool_results["image_generate"][:1]:  # First generated image
                        if isinstance(img, dict):
                            img_url = img.get("url") or img.get("image_url")
                            if img_url:
                                image_markdown.append(f"![Generated Image: {img.get('prompt', 'Uganda')}]({img_url})")
                
                context = "\n".join(context_parts) if context_parts else ""
                
                # Extract topic from user message (e.g., "write essay on computers" -> "computers")
                import re
                topic = request.message.lower().strip()
                # Try to extract topic after common phrases
                patterns = [
                    r"write\s+(?:an\s+)?essay\s+(?:on|about)\s+(.+)",
                    r"essay\s+(?:on|about)\s+(.+)",
                    r"write\s+(?:an\s+)?essay\s+(.+)",
                ]
                extracted = None
                for pattern in patterns:
                    match = re.search(pattern, topic)
                    if match:
                        extracted = match.group(1).strip()
                        break
                
                if extracted and len(extracted) > 2:
                    topic = extracted
                else:
                    # Fallback: remove common words and use the rest
                    words = topic.split()
                    filtered = [w for w in words if w not in ["write", "an", "essay", "on", "about", "the", "a"]]
                    topic = " ".join(filtered) if filtered else "the requested topic"
                
                # Generate essay
                image_text = "\n".join(image_markdown) if image_markdown else ""
                essay_prompt = f"""Write a comprehensive, well-structured essay about {topic}.

{context if context else f"Write about {topic} covering key aspects, current state, and important information."}

Requirements:
- Well-structured with introduction, body paragraphs, and conclusion
- Include relevant information from the research findings
- Professional academic tone
- Approximately 800-1200 words
- Include the following images in appropriate places:
{image_text if image_text else ""}

Write the complete essay now:"""
                
                try:
                    # Stream essay generation - OPEN WRITER AGENT TAB
                    essay_chunks = []
                    
                    # Open Writer Agent tab BEFORE generating content
                    from core.agent_stream_factory import get_agent_stream_handler
                    writer_handler = get_agent_stream_handler("writer", job_id, request.workspace_id)
                    
                    await events.connect()
                    # Open Writer tab immediately with distinct tab_id
                    await events.publish(job_id, "agent_stream", {
                        "agent": "writer",
                        "tab_id": writer_handler.tab_id,
                        "chunk": "",
                        "content": "âœï¸ **Writer Agent**\n\n_Generating essay content..._\n\n",
                        "type": "content",
                        "completed": False,
                        "workspace_id": request.workspace_id,
                        "metadata": {
                            "status": "running",
                            "action": "writing"
                        }
                    })
                    
                    async def stream_essay_chunk(chunk: str):
                        """Stream essay chunks to frontend AND Writer tab"""
                        essay_chunks.append(chunk)
                        accumulated = "".join(essay_chunks)
                        try:
                            await events.connect()
                            # Standard response_chunk for workspace
                            await events.publish(job_id, "response_chunk", {
                                "chunk": chunk,
                                "accumulated": accumulated
                            })
                            # Also stream to Writer agent tab
                            await writer_handler.stream_chunk(chunk, {
                                "word_count": len(accumulated.split()),
                                "status": "writing"
                            })
                        except Exception as e:
                            print(f"âš ï¸ Failed to stream essay chunk: {e}", flush=True)
                    
                    essay_content = await asyncio.wait_for(
                        deepseek_direct_service.generate_content(
                            prompt=essay_prompt,
                            system_prompt="You are an expert academic writer. Write comprehensive, well-researched essays with proper formatting.",
                            temperature=0.7,
                            max_tokens=3000,
                            use_reasoning=False,
                            stream=True,
                            stream_callback=stream_essay_chunk
                        ),
                        timeout=120.0
                    )
                    
                    if not essay_content or len(essay_content.strip()) < 100:
                        raise Exception("Generated essay content is too short or empty")
                    
                    # Insert images into essay if not already included
                    if image_markdown and len(image_markdown) > 0:
                        # Check if images are already in content
                        images_in_content = any(img_md in essay_content for img_md in image_markdown)
                        if not images_in_content:
                            # Insert first image after first paragraph
                            paragraphs = essay_content.split("\n\n")
                            if len(paragraphs) > 1:
                                paragraphs.insert(1, "\n".join(image_markdown))
                                essay_content = "\n\n".join(paragraphs)
                            else:
                                essay_content = essay_content + "\n\n" + "\n".join(image_markdown)
                    
                    response_parts.append("âœ“ Essay content generated successfully.")
                    
                    # Publish final essay content as response_chunk to ensure frontend gets it
                    try:
                        await events.connect()
                        await events.publish(job_id, "response_chunk", {
                            "chunk": "",  # Empty chunk to signal completion
                            "accumulated": essay_content,
                            "completed": True
                        })
                    except Exception as e:
                        print(f"âš ï¸ Failed to publish final essay: {e}", flush=True)
                    
                    # Emit content generation completed
                    try:
                        await events.log(job_id, f"âœ… Essay generated ({len(essay_content)} characters)", "info")
                        await events.publish(job_id, "stage_completed", {
                            "stage": "content_generation",
                            "metadata": {"length": len(essay_content)}
                        })
                    except:
                        pass
                except asyncio.TimeoutError:
                    print("âš ï¸ Essay generation timed out")
                    essay_content = f"# Essay About Uganda\n\n[Essay generation timed out. Please try again.]"
                    try:
                        await events.log(job_id, "âš ï¸ Essay generation timed out", "warning")
                    except:
                        pass
                except Exception as gen_error:
                    print(f"âš ï¸ Essay generation error: {gen_error}")
                    import traceback
                    traceback.print_exc()
                    essay_content = f"# Essay About Uganda\n\n[Error generating essay: {str(gen_error)[:200]}]"
                    try:
                        await events.log(job_id, f"âŒ Essay generation error: {str(gen_error)[:100]}", "error")
                    except:
                        pass
                
                # Detect and replace image placeholders
                import re
                image_placeholder_pattern = r'\[Image:([^\]]+)\]'
                placeholders = re.findall(image_placeholder_pattern, essay_content)
                
                if placeholders and len(placeholders) > 0:
                    try:
                        await events.log(job_id, f"ðŸ–¼ï¸ Detected {len(placeholders)} image placeholders, generating images...", "info")
                        
                        # Generate images for placeholders
                        for i, placeholder_desc in enumerate(placeholders[:3]):  # Limit to 3 images
                            try:
                                await events.log(job_id, f"ðŸŽ¨ Generating image {i+1}/{min(len(placeholders), 3)}: {placeholder_desc[:50]}...", "info")
                                
                                # Generate image
                                result = await execute_tool("image_generate", {
                                    "prompt": placeholder_desc.strip(),
                                    "size": "1024x1024"
                                }, request.workspace_id)
                                
                                if result.get("status") == "success" and result.get("image_url"):
                                    image_url = result["image_url"]
                                    # Replace first occurrence of this placeholder
                                    placeholder_full = f"[Image:{placeholder_desc}]"
                                    image_markdown = f"![Generated: {placeholder_desc.strip()}]({image_url})"
                                    essay_content = essay_content.replace(placeholder_full, image_markdown, 1)
                                    
                                    await events.log(job_id, f"âœ… Image {i+1} generated successfully", "info")
                                else:
                                    await events.log(job_id, f"âš ï¸ Failed to generate image {i+1}", "warning")
                            except Exception as img_error:
                                print(f"Image generation error for placeholder '{placeholder_desc}': {img_error}")
                                await events.log(job_id, f"âš ï¸ Image generation failed: {str(img_error)[:50]}", "warning")
                        
                        # Publish updated essay with images
                        try:
                            await events.publish(job_id, "response_chunk", {
                                "chunk": "",
                                "accumulated": essay_content,
                                "completed": True,
                                "images_added": True
                            })
                        except:
                            pass
                            
                    except Exception as e:
                        print(f"Error processing image placeholders: {e}")
                
            except Exception as e:
                print(f"Essay generation setup error: {e}")
                import traceback
                traceback.print_exc()
                essay_content = f"# Essay About Uganda\n\n[Error: {str(e)[:200]}]"
        
        # Second pass: Save file if we have content
        if essay_content:
            for step in plan:
                tool_name = step.get("tool")
                if tool_name in ["save_file", "write_file"]:
                    arguments = step.get("arguments", {})
                    file_path = arguments.get("path", "uganda_essay_with_images.md")
                    
                    # Update arguments with actual content
                    arguments["content"] = essay_content
                    arguments["path"] = file_path
                    
                    try:
                        # Emit file saving event
                        try:
                            await events.log(job_id, f"ðŸ’¾ Saving file: {file_path}...", "info")
                        except:
                            pass
                        
                        result = await execute_tool(tool_name, arguments, request.workspace_id)
                        if result.get("status") == "success":
                            essay_file_path = file_path
                            response_parts.append(f"âœ“ Essay saved to {file_path}")
                            recent_actions.append(f"{tool_name}: Saved essay to {file_path}")
                            
                            # Emit file created event for real-time updates
                            try:
                                await events.file_created(job_id, file_path, "markdown")
                                await events.log(job_id, f"âœ… File saved: {file_path}", "info")
                                await events.publish(job_id, "stage_completed", {
                                    "stage": "file_saving",
                                    "metadata": {"file_path": file_path}
                                })
                            except:
                                pass  # Don't fail if events fail
                        else:
                            response_parts.append(f"âš  Failed to save file: {result.get('error', 'Unknown error')}")
                    except Exception as tool_error:
                        print(f"Save file error: {tool_error}")
                        import traceback
                        traceback.print_exc()
                        response_parts.append(f"âš  Failed to save file: {str(tool_error)}")
                    break  # Only save once
        elif not tool_results:
            # No tools executed, no content to save
            response_parts.append("âš  No tools were executed, so no content was generated.")
        
        # Generate response text - include essay content if available
        if essay_content:
            # Use essay content as the main response
            response_text = essay_content
            # Add a brief summary at the end if file was saved
            if essay_file_path:
                response_text += f"\n\nâœ… Essay saved to {essay_file_path}"
        else:
            response_text = "\n".join(response_parts) if response_parts else "I've processed your request."
        
        # Include tool results in response for frontend to display (limit size to avoid huge responses)
        # tool_results is now a dict with tool_name as key
        try:
            for tool_name, result_data in list(tool_results.items())[:3]:  # Limit to 3 tool types
                if tool_name == "web_search" and result_data:
                    # Limit results to avoid huge base64 strings
                    limited_results = result_data[:3] if isinstance(result_data, list) else result_data
                    # Encode search results for frontend
                    search_data = {
                        "type": "web_search",
                        "query": "Search Results",
                        "results": limited_results
                    }
                    try:
                        search_json = json.dumps(search_data)
                        if len(search_json) > 10000:  # Limit JSON size
                            search_data["results"] = limited_results[:2] if isinstance(limited_results, list) else limited_results
                            search_json = json.dumps(search_data)
                        search_b64 = base64.b64encode(search_json.encode()).decode()
                        response_text += f"\n\n<!-- SEARCH_RESULTS_JSON_B64: {search_b64} -->"
                    except Exception as e:
                        print(f"Warning: Failed to encode search results: {e}")
                
                elif tool_name == "image_search" and result_data:
                    # Limit images to avoid huge base64 strings
                    limited_images = result_data[:2] if isinstance(result_data, list) else result_data
                    # Encode image search results for frontend
                    image_data = {
                        "type": "image_search",
                        "query": "Image Search Results",
                        "results": limited_images
                    }
                    try:
                        image_json = json.dumps(image_data)
                        if len(image_json) > 10000:  # Limit JSON size
                            image_data["results"] = limited_images[:1] if isinstance(limited_images, list) else limited_images
                            image_json = json.dumps(image_data)
                        image_b64 = base64.b64encode(image_json.encode()).decode()
                        response_text += f"\n\n<!-- IMAGE_SEARCH_JSON_B64: {image_b64} -->"
                    except Exception as e:
                        print(f"Warning: Failed to encode image results: {e}")
        except Exception as e:
            print(f"Warning: Failed to encode tool results: {e}")
        
        # Save to chat history (non-blocking, don't wait)
        try:
            # Don't wait for chat history save - it can be slow
            asyncio.create_task(chat_history_service.save_message(
                session_id=request.session_id,
                user_id=request.user_id,
                message=request.message,
                response=response_text[:1000],  # Truncate to avoid large responses
                metadata={
                    "reasoning": reasoning[:500] if reasoning else "",
                    "plan": plan,
                    "recent_actions": recent_actions,
                    "workspace_id": request.workspace_id,
                    "tool_results": {}  # Don't store full tool results in history
                }
            ))
        except Exception as e:
            print(f"Warning: Failed to save chat history: {e}")
        
        # Limit tool_results size to avoid huge responses
        limited_tool_results = {}
        for tool_name, result_data in tool_results.items():
            if tool_name == "web_search" and result_data:
                # Limit to first 3 results
                limited_tool_results[tool_name] = result_data[:3] if isinstance(result_data, list) else result_data
            elif tool_name == "image_search" and result_data:
                # Limit to first 2 images
                limited_tool_results[tool_name] = result_data[:2] if isinstance(result_data, list) else result_data
            elif tool_name == "image_generate" and result_data:
                # Limit to first 1 generated image
                limited_tool_results[tool_name] = result_data[:1] if isinstance(result_data, list) else result_data
        
        # Ensure essay content is in response
        final_response = response_text
        if essay_content and len(essay_content) > 100:
            # Use essay content as the main response
            final_response = essay_content
            if essay_file_path:
                final_response += f"\n\nâœ… Essay saved to {essay_file_path}"
            
            # Publish final essay as response_chunk if not already published
            try:
                await events.connect()
                await events.publish(job_id, "response_chunk", {
                    "chunk": "",
                    "accumulated": final_response,
                    "completed": True
                })
            except Exception as e:
                print(f"âš ï¸ Failed to publish final response: {e}", flush=True)
        elif not final_response or len(final_response.strip()) < 50:
            # Fallback if no good response
            final_response = response_text if response_text else "I've processed your request."
        
        response_data = {
            "response": final_response[:10000],  # Increased limit for essays
            "reasoning": reasoning[:1000] if reasoning else "",
            "plan": plan,
            "tool_results": limited_tool_results  # Include limited tool results
        }
        
        # If essay was created, include file path for auto-opening
        if essay_file_path:
            response_data["file_created"] = {
                "path": essay_file_path,
                "workspace_id": request.workspace_id
            }
        
        # Include job_id for frontend to subscribe to events
        response_data["job_id"] = job_id
        
        # Emit completion event
        try:
            await events.log(job_id, "âœ… Request processing completed!", "info")
            await events.publish(job_id, "stage_completed", {
                "stage": "complete",
                "metadata": {"file_path": essay_file_path or "none"}
            })
        except:
            pass
        
        print(f"âœ… Returning response (file: {essay_file_path or 'none'}, response_len: {len(response_text)}, job_id: {job_id})")
        
        # Ensure response is serializable
        try:
            json.dumps(response_data)  # Test serialization
            print("âœ… Response is serializable")
        except Exception as ser_error:
            print(f"âš ï¸ Response serialization error: {ser_error}")
            # Remove problematic data
            response_data["tool_results"] = {}
            response_data["response"] = response_text[:1000]
            response_data["plan"] = []
        
        print(f"âœ… About to return response_data")
        return response_data
        
    except Exception as e:
        import traceback
        error_trace = traceback.format_exc()
        print(f"âŒ Chat endpoint error: {e}")
        print(f"Traceback: {error_trace}")
        
        # Always return a response, never raise
        try:
            # Try self-healing (but don't wait for it)
            try:
                from services.self_healing import self_healing_system
                from services.rag_system import rag_system
                
                # Store error in RAG (async, but don't wait)
                asyncio.create_task(rag_system.store_solution(
                    problem=f"Chat endpoint error: {str(e)}",
                    solution="",
                    context={"traceback": error_trace[:500]},
                    category="errors"
                ))
            except:
                pass  # Don't fail on self-healing errors
        except:
            pass
        
        # Emit error event (job_id already exists from function start)
        try:
            await events.log(job_id, f"âŒ Error: {str(e)[:200]}", "error")
        except:
            pass
        
        # Return error response with job_id so frontend can connect to stream
        error_response = {
            "response": f"I apologize, but I encountered an error processing your request: {str(e)[:200]}. Please try again or rephrase your message.",
            "reasoning": f"Error: {str(e)[:200]}",
            "plan": [],
            "tool_results": {},
            "job_id": job_id  # Always include job_id so frontend can connect
        }
        
        # Include job_id if we have one
        if 'job_id' in locals():
            error_response["job_id"] = job_id
        
        return error_response

# ============================================================================
# SSE STREAMING ENDPOINT
# ============================================================================

@app.head("/api/stream/agent-actions")
async def stream_actions_head(request: Request):
    """HEAD handler for SSE endpoint to prevent 405 errors."""
    origin = request.headers.get("origin", "http://localhost:3000")
    allowed_origins = ["http://localhost:3000", "http://127.0.0.1:3000"]
    cors_origin = origin if origin in allowed_origins else "http://localhost:3000"
    
    return Response(
        content=b"",
        headers={
            "Access-Control-Allow-Origin": cors_origin,
            "Access-Control-Allow-Credentials": "true",
            "Access-Control-Expose-Headers": "*",
        },
        media_type="text/plain"
    )

async def event_generator(session_id: str = "default", job_id: Optional[str] = None):
    """Generate SSE events for agent actions and job progress."""
    import redis.asyncio as redis
    from core.config import settings
    
    # Send connection event
    yield {
        "event": "connected",
        "data": json.dumps({
            "status": "connected",
            "session_id": session_id,
            "job_id": job_id,
            "message": "Stream connected"
        })
    }
    
    # Subscribe to BOTH session and job channels for continuous chat
    pubsub = None
    redis_client = None
    try:
        redis_url = settings.REDIS_URL
        if redis_url.startswith("redis://redis:") and not os.path.exists("/.dockerenv"):
            redis_url = redis_url.replace("redis://redis:", "redis://localhost:")
        
        redis_client = redis.from_url(redis_url, decode_responses=True)
        pubsub = redis_client.pubsub()
        
        # Subscribe to session channel (receives ALL messages for this session)
        await pubsub.subscribe(f"session:{session_id}")
        print(f"ðŸ“¡ Subscribed to session:{session_id} for continuous chat", flush=True)
        
        # Also subscribe to job channel if provided
        if job_id:
            await pubsub.subscribe(f"job:{job_id}")
            print(f"ðŸ“¡ Also subscribed to job:{job_id}", flush=True)
            
            # Replay missed events from history (events that happened before connection)
            try:
                history_key = f"job:{job_id}:history"
                history_messages = await redis_client.lrange(history_key, 0, -1)
                if history_messages:
                    print(f"ðŸ“œ Replaying {len(history_messages)} missed events for job:{job_id}", flush=True)
                    for msg_json in history_messages:
                        try:
                            event_data = json.loads(msg_json)
                            yield {
                                "event": event_data.get("type", "log"),
                                "data": json.dumps(event_data.get("data", {}))
                            }
                        except Exception as replay_error:
                            print(f"âš ï¸ Error replaying event: {replay_error}", flush=True)
            except Exception as replay_err:
                print(f"âš ï¸ Failed to replay history: {replay_err}", flush=True)
                
    except Exception as e:
        print(f"âš ï¸ Failed to subscribe to channels: {e}", flush=True)
    
    # Event loop
    import asyncio
    keepalive_counter = 0
    
    try:
        while True:
            # Check for job events
            if pubsub:
                try:
                    # Get message with short timeout for responsiveness
                    message = await asyncio.wait_for(
                        pubsub.get_message(timeout=0.1),
                        timeout=0.1
                    )
                    if message:
                        if message["type"] == "message":
                            # Parse and forward the event
                            try:
                                event_data = json.loads(message["data"])
                                event_type = event_data.get("type", "log")
                                event_payload = event_data.get("data", {})
                                
                                print(f"ðŸ“¤ Sending SSE event [{event_type}]: {str(event_payload)[:100]}...", flush=True)
                                
                                yield {
                                    "event": event_type,
                                    "data": json.dumps(event_payload)
                                }
                            except json.JSONDecodeError as je:
                                print(f"âš ï¸ Failed to parse event JSON: {je}", flush=True)
                        elif message["type"] == "subscribe":
                            print(f"âœ… Subscribed to channel: {message.get('channel', 'unknown')}", flush=True)
                except asyncio.TimeoutError:
                    # No message, continue to keepalive check
                    pass
                except Exception as e:
                    print(f"âš ï¸ Error reading pubsub: {e}", flush=True)
                    import traceback
                    traceback.print_exc()
            
            # Send keepalive every 30 seconds
            keepalive_counter += 1
            if keepalive_counter >= 300:  # 300 * 0.1s = 30 seconds
                yield {
                    "event": "keepalive",
                    "data": json.dumps({
                        "timestamp": datetime.now().isoformat()
                    })
                }
                keepalive_counter = 0
            
            await asyncio.sleep(0.1)  # Short sleep for responsive event delivery
                
    finally:
        if pubsub:
            try:
                await pubsub.unsubscribe()
                await pubsub.close()
            except Exception as e:
                print(f"âš ï¸ Error closing pubsub: {e}", flush=True)
        if redis_client:
            try:
                await redis_client.close()
            except Exception as e:
                print(f"âš ï¸ Error closing redis client: {e}", flush=True)

@app.get("/api/stream/agent-actions")
async def stream_actions(request: Request, session_id: str = "default", job_id: Optional[str] = None):
    """SSE endpoint for streaming agent actions and job progress."""
    origin = request.headers.get("origin", "http://localhost:3000")
    allowed_origins = ["http://localhost:3000", "http://127.0.0.1:3000"]
    cors_origin = origin if origin in allowed_origins else "http://localhost:3000"
    
    return EventSourceResponse(
        event_generator(session_id, job_id),
        headers={
            "Access-Control-Allow-Origin": cors_origin,
            "Access-Control-Allow-Credentials": "true",
            "Access-Control-Expose-Headers": "*",
            "Cache-Control": "no-cache, no-transform",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )

# ============================================================================
# FILE UPLOAD ENDPOINTS
# ============================================================================

from fastapi import UploadFile, File

@app.post("/api/upload")
async def upload_file(
    file: UploadFile = File(...),
    workspace_id: str = "default",
    session_id: str = "default"
):
    """
    Upload a file to the workspace.
    Supports PDF, Excel, CSV, DOCX, images, and text files.
    Automatically extracts content for AI reference.
    """
    try:
        from services.file_upload_service import get_file_upload_service
        
        upload_service = get_file_upload_service()
        
        # Read file content
        content = await file.read()
        
        # Upload and process
        result = await upload_service.upload_file(
            file_content=content,
            filename=file.filename,
            workspace_id=workspace_id,
            session_id=session_id,
            extract_content=True
        )
        
        return result
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}


@app.get("/api/uploads/{workspace_id}")
async def list_uploads(workspace_id: str):
    """List all uploaded files in a workspace."""
    try:
        from services.file_upload_service import get_file_upload_service
        
        upload_service = get_file_upload_service()
        files = upload_service.list_uploads(workspace_id)
        
        return {"success": True, "files": files, "count": len(files)}
        
    except Exception as e:
        return {"success": False, "error": str(e)}


@app.get("/api/file/content/{workspace_id}/{filename}")
async def get_file_content(workspace_id: str, filename: str):
    """Get extracted content from an uploaded file."""
    try:
        from services.file_upload_service import get_file_upload_service
        from pathlib import Path
        
        upload_service = get_file_upload_service()
        file_path = Path("workspaces") / workspace_id / "uploads" / filename
        
        content = upload_service.get_file_content(file_path)
        
        if content:
            return {"success": True, "content": content[:10000], "truncated": len(content) > 10000}
        else:
            return {"success": False, "error": "No content extracted"}
            
    except Exception as e:
        return {"success": False, "error": str(e)}


# ============================================================================
# SPREADSHEET AND GRAPH ENDPOINTS
# ============================================================================

@app.post("/api/spreadsheet/create")
async def create_spreadsheet(request: dict):
    """Create an Excel or CSV file from data."""
    try:
        from services.spreadsheet_service import get_spreadsheet_service
        
        service = get_spreadsheet_service()
        
        data = request.get("data", [])
        filename = request.get("filename", "data.xlsx")
        workspace_id = request.get("workspace_id", "default")
        formulas = request.get("formulas")
        auto_formulas = request.get("auto_formulas")
        create_chart = request.get("chart")
        
        if filename.endswith(".csv"):
            result = service.write_csv(data, filename, workspace_id)
        else:
            result = service.write_excel(
                data, filename, 
                formulas=formulas,
                auto_formulas=auto_formulas,
                create_chart=create_chart,
                workspace_id=workspace_id
            )
        
        return result
        
    except Exception as e:
        return {"success": False, "error": str(e)}


@app.post("/api/graph/create")
async def create_graph(request: dict):
    """Create a chart/graph from data."""
    try:
        from services.graph_service import get_graph_service
        
        service = get_graph_service()
        
        data = request.get("data", [])
        chart_type = request.get("type", "bar")
        title = request.get("title", "Chart")
        x = request.get("x")
        y = request.get("y")
        workspace_id = request.get("workspace_id", "default")
        filename = request.get("filename")
        
        result = service.create_chart(
            data, chart_type, title, x, y,
            workspace_id=workspace_id,
            filename=filename
        )
        
        return result
        
    except Exception as e:
        return {"success": False, "error": str(e)}


@app.post("/api/analyze")
async def analyze_data(request: dict):
    """
    Analyze a dataset with AI-powered insights.
    
    Supports:
    - Basic analysis (schema, stats, quality)
    - LLM-powered insights (comprehensive, statistical, trends, quality)
    - Real-time streaming to UI
    """
    try:
        from services.dataset_analyzer import dataset_analyzer
        
        file_path = request.get("file_path")
        analysis_type = request.get("type", "comprehensive")
        job_id = request.get("job_id")
        workspace_id = request.get("workspace_id", "default")
        use_llm = request.get("use_llm", True)
        streaming = request.get("streaming", False)
        
        if not file_path:
            return {"success": False, "error": "file_path is required"}
        
        # Check if file exists
        from pathlib import Path
        if not Path(file_path).exists():
            # Try workspace path
            workspace_path = Path("workspaces") / workspace_id / "uploads" / file_path
            if workspace_path.exists():
                file_path = str(workspace_path)
            else:
                return {"success": False, "error": f"File not found: {file_path}"}
        
        if streaming and job_id:
            # Full streaming analysis with real-time updates
            result = await dataset_analyzer.analyze_streaming(
                file_path, job_id, workspace_id
            )
        elif use_llm:
            # LLM-powered analysis
            result = await dataset_analyzer.analyze_with_llm(
                file_path, analysis_type, job_id
            )
        else:
            # Basic analysis only
            result = await dataset_analyzer.analyze(file_path)
        
        return {"success": True, **result}
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}


# ============================================================================
# SKILLS ENDPOINTS (Already in file - keeping them)
# ============================================================================

@app.get("/api/skills")
async def list_skills():
    """List all available skills."""
    try:
        skills_manager = get_skills_manager()
        skills = skills_manager.list_skills()
        
        return {
            "skills": [
                {
                    "name": skill.name,
                    "description": skill.description,
                    "metadata": skill.metadata
                }
                for skill in skills
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/skills/{skill_name}")
async def get_skill(skill_name: str):
    """Get details of a specific skill."""
    try:
        skills_manager = get_skills_manager()
        skill = skills_manager.get_skill(skill_name)
        
        if not skill:
            raise HTTPException(status_code=404, detail=f"Skill '{skill_name}' not found")
        
        return {
            "name": skill.name,
            "description": skill.description,
            "instructions": skill.instructions,
            "metadata": skill.metadata,
            "path": str(skill.path)
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/skills/reload")
async def reload_skills():
    """Reload all skills from disk."""
    try:
        skills_manager = get_skills_manager()
        skills_manager.reload_skills()
        
        return {
            "status": "success",
            "message": "Skills reloaded successfully",
            "count": len(skills_manager.list_skills())
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/skills/for-task")
async def get_skills_for_task(task: str = Query(...)):
    """Get skills that might be relevant for a task."""
    try:
        skills_manager = get_skills_manager()
        relevant_skills = skills_manager.get_skills_for_task(task)
        
        return {
            "task": task,
            "skills": [
                {
                    "name": skill.name,
                    "description": skill.description
                }
                for skill in relevant_skills
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============================================================================
# DOCX CONVERSION ENDPOINT
# ============================================================================

class ConvertDocxRequest(BaseModel):
    content: str
    filename: str = "document"

@app.post("/api/files/{workspace_id}/convert/docx")
async def convert_to_docx(workspace_id: str, request: ConvertDocxRequest):
    """Convert markdown content to DOCX file with embedded images."""
    try:
        from services.docx_converter import convert_markdown_to_docx_enhanced
        
        docx_path = convert_markdown_to_docx_enhanced(request.content, request.filename, workspace_id=workspace_id)
        
        # Read the file and return it
        with open(docx_path, 'rb') as f:
            docx_content = f.read()
        
        # Clean up temp file
        import os
        try:
            os.unlink(docx_path)
        except:
            pass  # Ignore cleanup errors
        
        return Response(
            content=docx_content,
            media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            headers={
                "Content-Disposition": f'attachment; filename="{request.filename}.docx"'
            }
        )
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Failed to convert to DOCX: {str(e)}")


# ============================================================================
# REDIS & WORKER TEST ENDPOINTS
# ============================================================================

@app.get("/api/test/redis")
async def test_redis():
    """Test Redis connection."""
    try:
        from core.cache import cache
        client = await cache.get_client()
        result = await client.ping()
        queue_length = await client.llen("queue:objectives")
        return {
            "status": "connected" if result else "failed",
            "ping": result,
            "queue:objectives_length": queue_length,
            "message": "Redis is working!" if result else "Redis connection failed"
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "message": f"Redis connection error: {str(e)}"
        }

@app.post("/api/test/queue")
async def test_queue():
    """Test job queue by pushing a test job."""
    try:
        from core.queue import JobQueue
        job_id = await JobQueue.push(
            queue_name="objectives",
            data={
                "test": True,
                "message": "This is a test job"
            }
        )
        return {
            "status": "success",
            "job_id": job_id,
            "message": f"Test job {job_id} queued successfully"
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {
            "status": "error",
            "error": str(e),
            "message": f"Failed to queue job: {str(e)}"
        }

@app.get("/api/workers/status")
async def get_worker_status():
    """Get status of workers and queues."""
    try:
        from core.cache import cache
        client = await cache.get_client()
        
        # Check queue lengths
        objectives_queue = await client.llen("queue:objectives")
        content_queue = await client.llen("queue:content")
        search_queue = await client.llen("queue:search")
        tasks_queue = await client.llen("queue:tasks")
        
        # Check for recent jobs
        recent_jobs = []
        try:
            # Get recent job keys
            keys = await client.keys("job:*")
            for key in keys[:10]:  # Last 10 jobs
                job = await cache.get(key.replace("job:", ""))
                if job:
                    recent_jobs.append({
                        "job_id": job.get("job_id"),
                        "status": job.get("status"),
                        "queue": job.get("queue"),
                        "created_at": job.get("created_at")
                    })
        except:
            pass
        
        return {
            "status": "ok",
            "queues": {
                "objectives": {
                    "length": objectives_queue,
                    "status": "waiting" if objectives_queue > 0 else "empty"
                },
                "content": {
                    "length": content_queue,
                    "status": "waiting" if content_queue > 0 else "empty"
                },
                "search": {
                    "length": search_queue,
                    "status": "waiting" if search_queue > 0 else "empty"
                },
                "tasks": {
                    "length": tasks_queue,
                    "status": "waiting" if tasks_queue > 0 else "empty"
                }
            },
            "recent_jobs": recent_jobs,
            "message": "Workers are waiting for jobs. Start workers in separate terminals to process jobs."
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {
            "status": "error",
            "error": str(e),
            "message": f"Failed to get worker status: {str(e)}"
        }

# ============================================================================
# AGENT MONITORING & SELF-HEALING ENDPOINTS
# ============================================================================

@app.get("/api/agents/status")
async def get_agents_status():
    """Get status of all agents."""
    try:
        from services.agent_monitor import agent_monitor
        agents = await agent_monitor.get_all_agents()
        stats = await agent_monitor.get_agent_stats()
        
        return {
            "status": "ok",
            "agents": agents,
            "stats": stats
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {
            "status": "error",
            "error": str(e)
        }

@app.get("/api/agents/list")
async def list_agents():
    """Get a simple list of available agents for mentions."""
    try:
        from services.agent_monitor import agent_monitor
        agents = await agent_monitor.get_all_agents()
        
        # Format for frontend
        agent_list = []
        for agent_id, agent_data in agents.items():
            agent_list.append({
                "id": agent_id,
                "name": agent_data.get("name", agent_id),
                "type": agent_data.get("type", "unknown"),
                "health": agent_data.get("health", {}).get("status", "unknown"),
                "display_name": agent_data.get("name", agent_id).replace("_", " ").title()
            })
        
        # Also add some common/default agents if none found
        if not agent_list:
            agent_list = [
                {"id": "research", "name": "research", "type": "agent", "health": "healthy", "display_name": "Research Agent"},
                {"id": "writer", "name": "writer", "type": "agent", "health": "healthy", "display_name": "Writer Agent"},
                {"id": "editor", "name": "editor", "type": "agent", "health": "healthy", "display_name": "Editor Agent"},
                {"id": "planner", "name": "planner", "type": "agent", "health": "healthy", "display_name": "Planner Agent"},
            ]
        
        return {
            "status": "ok",
            "agents": agent_list
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        # Return default agents on error
        return {
            "status": "ok",
            "agents": [
                {"id": "research", "name": "research", "type": "agent", "health": "healthy", "display_name": "Research Agent", "description": "Handles web searches, academic research, and information gathering"},
                {"id": "writer", "name": "writer", "type": "agent", "health": "healthy", "display_name": "Writer Agent", "description": "Generates content, writes essays, and creates documents"},
                {"id": "editor", "name": "editor", "type": "agent", "health": "healthy", "display_name": "Editor Agent", "description": "Edits, refines, and improves existing content"},
                {"id": "planner", "name": "planner", "type": "agent", "health": "healthy", "display_name": "Planner Agent", "description": "Creates plans and organizes tasks"},
                {"id": "search", "name": "search", "type": "agent", "health": "healthy", "display_name": "Search Agent", "description": "Specialized in web and image searches"},
                {"id": "citation", "name": "citation", "type": "agent", "health": "healthy", "display_name": "Citation Agent", "description": "Handles academic citations and references"},
            ]
        }

@app.post("/api/agents/heal")
async def heal_agents():
    """Automatically heal unhealthy agents."""
    try:
        from services.agent_monitor import agent_monitor
        result = await agent_monitor.auto_heal_agents()
        
        return {
            "status": "ok",
            "healed": result["healed"],
            "failed": result["failed"],
            "total_checked": result["total_checked"]
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {
            "status": "error",
            "error": str(e)
        }

@app.post("/api/agents/generate")
async def generate_agent(request: Dict):
    """Generate a missing agent."""
    try:
        from services.self_healing import self_healing_system
        
        agent_name = request.get("agent_name")
        purpose = request.get("purpose", "")
        requirements = request.get("requirements", [])
        
        if not agent_name:
            raise HTTPException(status_code=400, detail="agent_name is required")
        
        result = await self_healing_system.generate_missing_agent(
            agent_name=agent_name,
            purpose=purpose or f"Handle {agent_name} tasks",
            requirements=requirements
        )
        
        return result
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/jobs/{job_id}")
async def get_job_status(job_id: str):
    """Get status of a job."""
    try:
        from core.queue import JobQueue
        job = await JobQueue.get_status(job_id)
        
        if not job:
            raise HTTPException(status_code=404, detail="Job not found")
        
        return {
            "status": "ok",
            "job": job
        }
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/rag/search")
async def search_rag(query: str, category: str = "solutions", top_k: int = 5):
    """Search RAG knowledge base."""
    try:
        from services.rag_system import rag_system
        results = await rag_system.retrieve_similar(
            query=query,
            category=category,
            top_k=top_k
        )
        
        return {
            "status": "ok",
            "results": results
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))
